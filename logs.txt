* 
* ==> Audit <==
* |------------|--------------------------------|------------|----------|---------|----------------------|----------------------|
|  Command   |              Args              |  Profile   |   User   | Version |      Start Time      |       End Time       |
|------------|--------------------------------|------------|----------|---------|----------------------|----------------------|
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 09 Jun 23 15:46 EEST | 09 Jun 23 15:46 EEST |
| addons     | enable ingress                 | minikube   | taabaaf1 | v1.30.1 | 09 Jun 23 15:47 EEST | 09 Jun 23 15:47 EEST |
| start      | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 09 Jun 23 16:42 EEST | 09 Jun 23 16:43 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 09 Jun 23 16:45 EEST | 09 Jun 23 16:45 EEST |
| ip         | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 09 Jun 23 16:45 EEST | 09 Jun 23 16:45 EEST |
| stop       | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 09 Jun 23 16:45 EEST | 09 Jun 23 16:46 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 09 Jun 23 16:46 EEST | 09 Jun 23 16:46 EEST |
| start      | -p minikube                    | minikube   | taabaaf1 | v1.30.1 | 09 Jun 23 16:47 EEST | 09 Jun 23 16:48 EEST |
| docker-env |                                | minikube   | taabaaf1 | v1.30.1 | 09 Jun 23 18:10 EEST | 09 Jun 23 18:10 EEST |
| start      | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 16:39 EEST | 12 Jun 23 16:40 EEST |
| docker-env |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 16:44 EEST | 12 Jun 23 16:44 EEST |
| config     | set memory 8192                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 16:53 EEST | 12 Jun 23 16:53 EEST |
| config     | set cpus 4                     | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 16:53 EEST | 12 Jun 23 16:53 EEST |
| stop       | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 16:56 EEST | 12 Jun 23 16:56 EEST |
| start      | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 16:56 EEST |                      |
| start      | -p cluster-2 --memory=4096     | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 16:57 EEST | 12 Jun 23 16:57 EEST |
|            | --cpus=2                       |            |          |         |                      |                      |
| docker-env |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 16:59 EEST | 12 Jun 23 16:59 EEST |
| stop       | -p minikube                    | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:14 EEST | 12 Jun 23 17:14 EEST |
| docker-env |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:16 EEST |                      |
| docker-env |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:21 EEST |                      |
| start      |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:22 EEST | 12 Jun 23 17:23 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:23 EEST | 12 Jun 23 17:23 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:23 EEST | 12 Jun 23 17:23 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:24 EEST | 12 Jun 23 17:24 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:26 EEST | 12 Jun 23 17:26 EEST |
| stop       | -p minikube                    | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:27 EEST | 12 Jun 23 17:27 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:28 EEST |                      |
| start      | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 17:28 EEST | 12 Jun 23 17:29 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 12 Jun 23 17:29 EEST |                      |
| ip         |                                | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 17:31 EEST | 12 Jun 23 17:31 EEST |
| start      |                                | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 17:34 EEST | 12 Jun 23 17:34 EEST |
| ip         |                                | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 17:34 EEST | 12 Jun 23 17:34 EEST |
| ip         |                                | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 17:34 EEST | 12 Jun 23 17:34 EEST |
| docker-env |                                | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 17:35 EEST | 12 Jun 23 17:35 EEST |
| ip         |                                | cluster-2  | taabaaf1 | v1.30.1 | 12 Jun 23 17:35 EEST | 12 Jun 23 17:35 EEST |
| start      | -p minikube                    | minikube   | taabaaf1 | v1.30.1 | 13 Jun 23 15:07 EEST | 13 Jun 23 15:08 EEST |
| docker-env |                                | minikube   | taabaaf1 | v1.30.1 | 16 Jun 23 15:04 EEST |                      |
| start      |                                | minikube   | taabaaf1 | v1.30.1 | 16 Jun 23 15:04 EEST | 16 Jun 23 15:05 EEST |
| start      | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 16 Jun 23 15:06 EEST | 16 Jun 23 15:07 EEST |
| docker-env |                                | minikube   | taabaaf1 | v1.30.1 | 16 Jun 23 15:07 EEST | 16 Jun 23 15:07 EEST |
| ip         |                                | minikube   | taabaaf1 | v1.30.1 | 16 Jun 23 15:18 EEST | 16 Jun 23 15:18 EEST |
| ip         | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 16 Jun 23 15:19 EEST | 16 Jun 23 15:19 EEST |
| start      | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 15:50 EEST | 18 Jun 23 15:50 EEST |
| ip         |                                | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 15:54 EEST | 18 Jun 23 15:54 EEST |
| ip         |                                | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 16:01 EEST | 18 Jun 23 16:01 EEST |
| start      | --nodes 2 -p audit-node        | audit-node | taabaaf1 | v1.30.1 | 18 Jun 23 16:12 EEST |                      |
| service    |                                | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 17:08 EEST |                      |
| service    | jaeger-query                   | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 17:09 EEST | 18 Jun 23 17:09 EEST |
| addons     | enable ingress                 | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 17:10 EEST | 18 Jun 23 17:11 EEST |
| ip         |                                | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 17:11 EEST | 18 Jun 23 17:11 EEST |
| ip         | -p cluster-2                   | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 17:18 EEST | 18 Jun 23 17:18 EEST |
| service    | jaeger-query                   | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 17:27 EEST | 18 Jun 23 17:27 EEST |
| docker-env |                                | cluster-2  | taabaaf1 | v1.30.1 | 18 Jun 23 17:29 EEST | 18 Jun 23 17:29 EEST |
| start      |                                | cluster-2  | taabaaf1 | v1.30.1 | 21 Jun 23 16:27 EEST |                      |
| docker-env |                                | cluster-2  | taabaaf1 | v1.30.1 | 21 Jun 23 16:27 EEST | 21 Jun 23 16:27 EEST |
| start      | -p cluster-3                   | cluster-3  | taabaaf1 | v1.30.1 | 21 Jun 23 16:29 EEST |                      |
| start      |                                | cluster-2  | taabaaf1 | v1.30.1 | 21 Jun 23 16:30 EEST | 21 Jun 23 16:31 EEST |
| docker-env |                                | cluster-2  | taabaaf1 | v1.30.1 | 21 Jun 23 16:33 EEST | 21 Jun 23 16:33 EEST |
| docker-env |                                | cluster-2  | taabaaf1 | v1.30.1 | 21 Jun 23 16:35 EEST | 21 Jun 23 16:35 EEST |
| start      | -p cluster-3                   | cluster-3  | taabaaf1 | v1.30.1 | 21 Jun 23 16:36 EEST | 21 Jun 23 16:37 EEST |
|------------|--------------------------------|------------|----------|---------|----------------------|----------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/06/21 16:36:35
Running on machine: UM01512
Binary: Built with gc go1.20.2 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0621 16:36:35.159020   19000 out.go:296] Setting OutFile to fd 1 ...
I0621 16:36:35.159878   19000 out.go:348] isatty.IsTerminal(1) = true
I0621 16:36:35.159881   19000 out.go:309] Setting ErrFile to fd 2...
I0621 16:36:35.159886   19000 out.go:348] isatty.IsTerminal(2) = true
I0621 16:36:35.160497   19000 root.go:336] Updating PATH: /Users/taabaaf1/.minikube/bin
I0621 16:36:35.160678   19000 oci.go:567] shell is pointing to dockerd inside minikube. will unset to use host
I0621 16:36:35.165045   19000 out.go:303] Setting JSON to false
I0621 16:36:35.197966   19000 start.go:125] hostinfo: {"hostname":"UM01512","uptime":266373,"bootTime":1687088222,"procs":449,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.4","kernelVersion":"22.5.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"1f2f05ce-799c-51e3-b026-3d02788215b5"}
W0621 16:36:35.198077   19000 start.go:133] gopshost.Virtualization returned error: not implemented yet
I0621 16:36:35.205649   19000 out.go:177] 😄  [cluster-3] minikube v1.30.1 on Darwin 13.4 (arm64)
I0621 16:36:35.217310   19000 out.go:177]     ▪ MINIKUBE_ACTIVE_DOCKERD=cluster-2
I0621 16:36:35.213087   19000 notify.go:220] Checking for updates...
I0621 16:36:35.229973   19000 config.go:182] Loaded profile config "audit-node": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I0621 16:36:35.230062   19000 config.go:182] Loaded profile config "cluster-2": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I0621 16:36:35.232009   19000 config.go:182] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I0621 16:36:35.232890   19000 driver.go:375] Setting default libvirt URI to qemu:///system
I0621 16:36:35.233055   19000 global.go:111] Querying for installed drivers using PATH=/Users/taabaaf1/.minikube/bin:/Users/taabaaf1/.rd/bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin
I0621 16:36:35.233807   19000 global.go:122] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0621 16:36:35.233869   19000 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "docker-machine-driver-vmware": executable file not found in $PATH Reason: Fix:Install docker-machine-driver-vmware Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
W0621 16:36:35.357961   19000 podman.go:138] podman returned error: exit status 125
I0621 16:36:35.358002   19000 global.go:122] podman default: true priority: 3, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:"podman version --format {{.Server.Version}}" exit status 125: Error: failed to connect: dial tcp 127.0.0.1:60830: connect: connection refused Reason: Fix: Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0621 16:36:35.358246   19000 global.go:122] qemu2 default: true priority: 7, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0621 16:36:35.358256   19000 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0621 16:36:35.392192   19000 virtualbox.go:136] virtual box version: 7.0.6_BETA4r155176
I0621 16:36:35.392215   19000 global.go:122] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:7.0.6_BETA4r155176
}
I0621 16:36:35.392226   19000 global.go:122] vmwarefusion default: false priority: 1, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:the 'vmwarefusion' driver is no longer available Reason: Fix:Switch to the newer 'vmware' driver by using '--driver=vmware'. This may require first deleting your existing cluster Doc:https://minikube.sigs.k8s.io/docs/drivers/vmware/ Version:}
W0621 16:36:35.455692   19000 docker.go:168] docker version returned error: exit status 1
I0621 16:36:35.455722   19000 global.go:122] docker default: true priority: 9, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:"docker version --format {{.Server.Os}}-{{.Server.Version}}:{{.Server.Platform.Name}}" exit status 1: Cannot connect to the Docker daemon at unix:///Users/taabaaf1/.colima/default/docker.sock. Is the docker daemon running? Reason:PROVIDER_DOCKER_NOT_RUNNING Fix:Start the Docker service Doc:https://minikube.sigs.k8s.io/docs/drivers/docker/ Version:}
I0621 16:36:35.455808   19000 global.go:122] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0621 16:36:35.455834   19000 driver.go:310] not recommending "ssh" due to default: false
I0621 16:36:35.455836   19000 driver.go:305] not recommending "podman" due to health: "podman version --format {{.Server.Version}}" exit status 125: Error: failed to connect: dial tcp 127.0.0.1:60830: connect: connection refused
I0621 16:36:35.455838   19000 driver.go:305] not recommending "docker" due to health: "docker version --format {{.Server.Os}}-{{.Server.Version}}:{{.Server.Platform.Name}}" exit status 1: Cannot connect to the Docker daemon at unix:///Users/taabaaf1/.colima/default/docker.sock. Is the docker daemon running?
I0621 16:36:35.455857   19000 driver.go:345] Picked: qemu2
I0621 16:36:35.455861   19000 driver.go:346] Alternatives: [virtualbox ssh]
I0621 16:36:35.455869   19000 driver.go:347] Rejects: [hyperkit vmware podman vmwarefusion docker parallels]
I0621 16:36:35.464403   19000 out.go:177] ✨  Automatically selected the qemu2 driver. Other choices: virtualbox, ssh
I0621 16:36:35.468472   19000 start.go:295] selected driver: qemu2
I0621 16:36:35.468610   19000 start.go:870] validating driver "qemu2" against <nil>
I0621 16:36:35.468617   19000 start.go:881] status for qemu2: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0621 16:36:35.469038   19000 start_flags.go:305] no existing cluster config was found, will generate one from the flags 
I0621 16:36:35.472793   19000 out.go:177] 🌐  Automatically selected the socket_vmnet network
I0621 16:36:35.477705   19000 start_flags.go:901] Wait components to verify : map[apiserver:true system_pods:true]
I0621 16:36:35.477854   19000 cni.go:84] Creating CNI manager for ""
I0621 16:36:35.477982   19000 cni.go:157] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0621 16:36:35.477988   19000 start_flags.go:314] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0621 16:36:35.477996   19000 start_flags.go:319] config:
{Name:cluster-3 KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 Memory:8192 CPUs:4 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:cluster-3 Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP:}
I0621 16:36:35.478102   19000 iso.go:125] acquiring lock: {Name:mk452866b866cbfeee7b670fe9b702d2e647cd06 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0621 16:36:35.483379   19000 out.go:177] 👍  Starting control plane node cluster-3 in cluster cluster-3
I0621 16:36:35.487517   19000 preload.go:132] Checking if preload exists for k8s version v1.26.3 and runtime docker
I0621 16:36:35.487561   19000 preload.go:148] Found local preload: /Users/taabaaf1/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.3-docker-overlay2-arm64.tar.lz4
I0621 16:36:35.488080   19000 cache.go:57] Caching tarball of preloaded images
I0621 16:36:35.488163   19000 preload.go:174] Found /Users/taabaaf1/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0621 16:36:35.488176   19000 cache.go:60] Finished verifying existence of preloaded tar for  v1.26.3 on docker
I0621 16:36:35.488231   19000 profile.go:148] Saving config to /Users/taabaaf1/.minikube/profiles/cluster-3/config.json ...
I0621 16:36:35.488243   19000 lock.go:35] WriteFile acquiring /Users/taabaaf1/.minikube/profiles/cluster-3/config.json: {Name:mk7132d7380f22f0bdbc573e993668aea4fbd146 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:36:35.489227   19000 cache.go:193] Successfully downloaded all kic artifacts
I0621 16:36:35.489234   19000 start.go:364] acquiring machines lock for cluster-3: {Name:mk98bfaea141fcfccd67bbd02d7bb1bf2c180b43 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0621 16:36:35.489268   19000 start.go:368] acquired machines lock for "cluster-3" in 30.584µs
I0621 16:36:35.489276   19000 start.go:93] Provisioning new machine with config: &{Name:cluster-3 KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.30.1-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 Memory:8192 CPUs:4 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:cluster-3 Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP:} &{Name: IP: Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0621 16:36:35.489299   19000 start.go:125] createHost starting for "" (driver="qemu2")
I0621 16:36:35.496617   19000 out.go:204] 🔥  Creating qemu2 VM (CPUs=4, Memory=8192MB, Disk=20000MB) ...
I0621 16:36:35.695369   19000 start.go:159] libmachine.API.Create for "cluster-3" (driver="qemu2")
I0621 16:36:35.695550   19000 client.go:168] LocalClient.Create starting
I0621 16:36:35.696171   19000 main.go:141] libmachine: Reading certificate data from /Users/taabaaf1/.minikube/certs/ca.pem
I0621 16:36:35.696341   19000 main.go:141] libmachine: Decoding PEM data...
I0621 16:36:35.696354   19000 main.go:141] libmachine: Parsing certificate...
I0621 16:36:35.697060   19000 main.go:141] libmachine: Reading certificate data from /Users/taabaaf1/.minikube/certs/cert.pem
I0621 16:36:35.697199   19000 main.go:141] libmachine: Decoding PEM data...
I0621 16:36:35.697205   19000 main.go:141] libmachine: Parsing certificate...
I0621 16:36:35.698869   19000 main.go:141] libmachine: Downloading /Users/taabaaf1/.minikube/cache/boot2docker.iso from file:///Users/taabaaf1/.minikube/cache/iso/arm64/minikube-v1.30.1-arm64.iso...
I0621 16:36:35.943213   19000 main.go:141] libmachine: Creating SSH key...
I0621 16:36:36.052473   19000 main.go:141] libmachine: Creating Disk image...
I0621 16:36:36.052481   19000 main.go:141] libmachine: Creating 20000 MB hard disk image...
I0621 16:36:36.063840   19000 main.go:141] libmachine: executing: qemu-img convert -f raw -O qcow2 /Users/taabaaf1/.minikube/machines/cluster-3/disk.qcow2.raw /Users/taabaaf1/.minikube/machines/cluster-3/disk.qcow2
I0621 16:36:36.127576   19000 main.go:141] libmachine: STDOUT: 
I0621 16:36:36.127598   19000 main.go:141] libmachine: STDERR: 
I0621 16:36:36.127671   19000 main.go:141] libmachine: executing: qemu-img resize /Users/taabaaf1/.minikube/machines/cluster-3/disk.qcow2 +20000M
I0621 16:36:36.141299   19000 main.go:141] libmachine: STDOUT: Image resized.

I0621 16:36:36.141313   19000 main.go:141] libmachine: STDERR: 
I0621 16:36:36.141342   19000 main.go:141] libmachine: DONE writing to /Users/taabaaf1/.minikube/machines/cluster-3/disk.qcow2.raw and /Users/taabaaf1/.minikube/machines/cluster-3/disk.qcow2
I0621 16:36:36.141345   19000 main.go:141] libmachine: Starting QEMU VM...
I0621 16:36:36.141388   19000 main.go:141] libmachine: executing: /opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client /opt/homebrew/var/run/socket_vmnet qemu-system-aarch64 -M virt -cpu host -drive file=/opt/homebrew/Cellar/qemu/8.0.0/share/qemu/edk2-aarch64-code.fd,readonly=on,format=raw,if=pflash -display none -accel hvf -m 8192 -smp 4 -boot d -cdrom /Users/taabaaf1/.minikube/machines/cluster-3/boot2docker.iso -qmp unix:/Users/taabaaf1/.minikube/machines/cluster-3/monitor,server,nowait -pidfile /Users/taabaaf1/.minikube/machines/cluster-3/qemu.pid -device virtio-net-pci,netdev=net0,mac=d2:6e:28:d7:5e:56 -netdev socket,id=net0,fd=3 -daemonize /Users/taabaaf1/.minikube/machines/cluster-3/disk.qcow2
I0621 16:36:36.221197   19000 main.go:141] libmachine: STDOUT: 
I0621 16:36:36.221213   19000 main.go:141] libmachine: STDERR: 
I0621 16:36:36.221218   19000 main.go:141] libmachine: Attempt 0
I0621 16:36:36.221236   19000 main.go:141] libmachine: Searching for d2:6e:28:d7:5e:56 in /var/db/dhcpd_leases ...
I0621 16:36:36.225158   19000 main.go:141] libmachine: Found 53 entries in /var/db/dhcpd_leases!
I0621 16:36:36.225179   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.53 HWAddress:86:a2:b9:b:1b:95 ID:1,86:a2:b9:b:1b:95 Lease:0x6493022a}
I0621 16:36:36.225186   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.52 HWAddress:ba:94:7c:fc:ef:eb ID:1,ba:94:7c:fc:ef:eb Lease:0x6492fcfe}
I0621 16:36:36.225191   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.51 HWAddress:ae:12:df:b5:f5:83 ID:1,ae:12:df:b5:f5:83 Lease:0x648da176}
I0621 16:36:36.225196   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.50 HWAddress:a2:e3:b4:1:f4:e7 ID:1,a2:e3:b4:1:f4:e7 Lease:0x6482e5e0}
I0621 16:36:36.225201   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.49 HWAddress:4e:11:49:3f:6b:b4 ID:1,4e:11:49:3f:6b:b4 Lease:0x647f52e2}
I0621 16:36:36.225205   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.48 HWAddress:96:d4:55:90:d9:c2 ID:1,96:d4:55:90:d9:c2 Lease:0x647850dd}
I0621 16:36:36.225209   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.47 HWAddress:ea:52:59:d8:7:5 ID:1,ea:52:59:d8:7:5 Lease:0x6467a61a}
I0621 16:36:36.225214   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.46 HWAddress:2:b5:af:c4:df:d5 ID:1,2:b5:af:c4:df:d5 Lease:0x64676fda}
I0621 16:36:36.225225   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.45 HWAddress:72:92:af:ec:b3:98 ID:1,72:92:af:ec:b3:98 Lease:0x64676c2c}
I0621 16:36:36.225230   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.44 HWAddress:62:f9:1c:d8:ad:d ID:1,62:f9:1c:d8:ad:d Lease:0x6452a5af}
I0621 16:36:36.225237   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.43 HWAddress:2:e0:c4:f4:b1:6c ID:1,2:e0:c4:f4:b1:6c Lease:0x645298cf}
I0621 16:36:36.225242   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.42 HWAddress:72:9a:65:1d:7:8f ID:1,72:9a:65:1d:7:8f Lease:0x64529727}
I0621 16:36:36.225246   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.41 HWAddress:ae:61:59:6d:f1:c9 ID:1,ae:61:59:6d:f1:c9 Lease:0x64526e21}
I0621 16:36:36.225251   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.40 HWAddress:6e:2f:9c:f4:b6:96 ID:1,6e:2f:9c:f4:b6:96 Lease:0x6450ffe5}
I0621 16:36:36.225255   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.39 HWAddress:72:12:77:32:47:49 ID:1,72:12:77:32:47:49 Lease:0x64523e03}
I0621 16:36:36.225259   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.38 HWAddress:d2:25:e3:2d:31:49 ID:1,d2:25:e3:2d:31:49 Lease:0x6451b25b}
I0621 16:36:36.225264   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.37 HWAddress:5e:a9:62:e6:82:b3 ID:1,5e:a9:62:e6:82:b3 Lease:0x64517e96}
I0621 16:36:36.225268   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.36 HWAddress:9a:5c:d0:fc:fb:3e ID:1,9a:5c:d0:fc:fb:3e Lease:0x64500013}
I0621 16:36:36.225273   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.35 HWAddress:ba:c1:75:8c:94:13 ID:1,ba:c1:75:8c:94:13 Lease:0x644ff9cf}
I0621 16:36:36.225277   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.34 HWAddress:b6:4e:7e:54:f4:e6 ID:1,b6:4e:7e:54:f4:e6 Lease:0x644fb70e}
I0621 16:36:36.225281   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.33 HWAddress:d2:c2:9:16:42:e6 ID:1,d2:c2:9:16:42:e6 Lease:0x644fb655}
I0621 16:36:36.225286   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.32 HWAddress:22:6d:e0:99:d2:87 ID:1,22:6d:e0:99:d2:87 Lease:0x644706a8}
I0621 16:36:36.225290   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.31 HWAddress:ea:5:30:30:d2:15 ID:1,ea:5:30:30:d2:15 Lease:0x6446fd38}
I0621 16:36:36.225294   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.30 HWAddress:82:2e:a7:f9:69:6 ID:1,82:2e:a7:f9:69:6 Lease:0x6446fa8c}
I0621 16:36:36.225299   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.29 HWAddress:4a:ab:16:b1:c9:a9 ID:1,4a:ab:16:b1:c9:a9 Lease:0x6446f6c9}
I0621 16:36:36.225311   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.28 HWAddress:e6:bd:18:1f:e7:4c ID:1,e6:bd:18:1f:e7:4c Lease:0x6446f4d5}
I0621 16:36:36.225315   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.27 HWAddress:ba:22:23:ac:61:85 ID:1,ba:22:23:ac:61:85 Lease:0x6446f41c}
I0621 16:36:36.225320   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.26 HWAddress:2a:8e:ef:28:96:fb ID:1,2a:8e:ef:28:96:fb Lease:0x6446ef65}
I0621 16:36:36.225324   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.25 HWAddress:72:dd:28:42:7b:d3 ID:1,72:dd:28:42:7b:d3 Lease:0x6446eef4}
I0621 16:36:36.225328   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.24 HWAddress:96:89:6b:d4:8:e1 ID:1,96:89:6b:d4:8:e1 Lease:0x6446ee99}
I0621 16:36:36.225333   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.23 HWAddress:4a:e6:7f:50:3f:25 ID:1,4a:e6:7f:50:3f:25 Lease:0x6446ecb5}
I0621 16:36:36.225337   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.22 HWAddress:2e:85:5c:8c:98:7d ID:1,2e:85:5c:8c:98:7d Lease:0x6446ebcf}
I0621 16:36:36.225341   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.21 HWAddress:16:e4:e:cc:b9:83 ID:1,16:e4:e:cc:b9:83 Lease:0x6446e7cb}
I0621 16:36:36.225346   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.20 HWAddress:8a:52:7c:ba:6d:82 ID:1,8a:52:7c:ba:6d:82 Lease:0x644287e4}
I0621 16:36:36.225350   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.19 HWAddress:1e:3c:4e:37:9:ac ID:1,1e:3c:4e:37:9:ac Lease:0x64428247}
I0621 16:36:36.225355   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.18 HWAddress:8e:2c:13:85:8c:a4 ID:1,8e:2c:13:85:8c:a4 Lease:0x64428110}
I0621 16:36:36.225359   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.17 HWAddress:66:53:7:ca:f2:b0 ID:1,66:53:7:ca:f2:b0 Lease:0x64428002}
I0621 16:36:36.225363   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.16 HWAddress:f6:5e:66:47:59:47 ID:1,f6:5e:66:47:59:47 Lease:0x64427baa}
I0621 16:36:36.225368   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.15 HWAddress:4e:63:4b:ba:4f:fb ID:1,4e:63:4b:ba:4f:fb Lease:0x644277cc}
I0621 16:36:36.225372   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.14 HWAddress:92:e8:71:b6:c:e5 ID:1,92:e8:71:b6:c:e5 Lease:0x644270d0}
I0621 16:36:36.225380   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.13 HWAddress:ce:f3:94:6a:24:ce ID:1,ce:f3:94:6a:24:ce Lease:0x644264a6}
I0621 16:36:36.225389   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.12 HWAddress:ca:db:87:22:4:58 ID:1,ca:db:87:22:4:58 Lease:0x644241fe}
I0621 16:36:36.225393   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.11 HWAddress:16:3c:2d:1b:40:d3 ID:1,16:3c:2d:1b:40:d3 Lease:0x64412889}
I0621 16:36:36.225398   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.10 HWAddress:fa:a7:aa:32:8d:43 ID:1,fa:a7:aa:32:8d:43 Lease:0x6441284e}
I0621 16:36:36.225402   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.9 HWAddress:7e:45:19:59:e9:63 ID:1,7e:45:19:59:e9:63 Lease:0x64412806}
I0621 16:36:36.225406   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.8 HWAddress:3a:9d:fb:67:eb:ba ID:1,3a:9d:fb:67:eb:ba Lease:0x64411f96}
I0621 16:36:36.225411   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.7 HWAddress:76:a7:40:98:e9:b7 ID:1,76:a7:40:98:e9:b7 Lease:0x643a7c51}
I0621 16:36:36.225415   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.6 HWAddress:b2:2b:36:17:ec:7a ID:1,b2:2b:36:17:ec:7a Lease:0x642bf388}
I0621 16:36:36.225420   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.5 HWAddress:da:99:b2:56:8d:76 ID:1,da:99:b2:56:8d:76 Lease:0x64297564}
I0621 16:36:36.225424   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.4 HWAddress:16:a2:dc:3e:ee:a3 ID:1,16:a2:dc:3e:ee:a3 Lease:0x6429708c}
I0621 16:36:36.225434   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.3 HWAddress:16:94:64:d9:c7:c8 ID:1,16:94:64:d9:c7:c8 Lease:0x64296d6a}
I0621 16:36:36.225439   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:7a:19:8:2a:35:4c ID:1,7a:19:8:2a:35:4c Lease:0x64284dd1}
I0621 16:36:36.225443   19000 main.go:141] libmachine: dhcp entry: {Name:lima-0 IPAddress:192.168.205.2 HWAddress:52:55:55:63:f9:7d ID:1,52:55:55:63:f9:7d Lease:0x64514587}
I0621 16:36:38.225912   19000 main.go:141] libmachine: Attempt 1
I0621 16:36:38.225958   19000 main.go:141] libmachine: Searching for d2:6e:28:d7:5e:56 in /var/db/dhcpd_leases ...
I0621 16:36:38.226231   19000 main.go:141] libmachine: Found 53 entries in /var/db/dhcpd_leases!
I0621 16:36:38.226249   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.53 HWAddress:86:a2:b9:b:1b:95 ID:1,86:a2:b9:b:1b:95 Lease:0x6493022a}
I0621 16:36:38.226268   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.52 HWAddress:ba:94:7c:fc:ef:eb ID:1,ba:94:7c:fc:ef:eb Lease:0x6492fcfe}
I0621 16:36:38.226281   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.51 HWAddress:ae:12:df:b5:f5:83 ID:1,ae:12:df:b5:f5:83 Lease:0x648da176}
I0621 16:36:38.226295   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.50 HWAddress:a2:e3:b4:1:f4:e7 ID:1,a2:e3:b4:1:f4:e7 Lease:0x6482e5e0}
I0621 16:36:38.226308   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.49 HWAddress:4e:11:49:3f:6b:b4 ID:1,4e:11:49:3f:6b:b4 Lease:0x647f52e2}
I0621 16:36:38.226322   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.48 HWAddress:96:d4:55:90:d9:c2 ID:1,96:d4:55:90:d9:c2 Lease:0x647850dd}
I0621 16:36:38.226335   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.47 HWAddress:ea:52:59:d8:7:5 ID:1,ea:52:59:d8:7:5 Lease:0x6467a61a}
I0621 16:36:38.226355   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.46 HWAddress:2:b5:af:c4:df:d5 ID:1,2:b5:af:c4:df:d5 Lease:0x64676fda}
I0621 16:36:38.226369   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.45 HWAddress:72:92:af:ec:b3:98 ID:1,72:92:af:ec:b3:98 Lease:0x64676c2c}
I0621 16:36:38.226382   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.44 HWAddress:62:f9:1c:d8:ad:d ID:1,62:f9:1c:d8:ad:d Lease:0x6452a5af}
I0621 16:36:38.226401   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.43 HWAddress:2:e0:c4:f4:b1:6c ID:1,2:e0:c4:f4:b1:6c Lease:0x645298cf}
I0621 16:36:38.226421   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.42 HWAddress:72:9a:65:1d:7:8f ID:1,72:9a:65:1d:7:8f Lease:0x64529727}
I0621 16:36:38.226435   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.41 HWAddress:ae:61:59:6d:f1:c9 ID:1,ae:61:59:6d:f1:c9 Lease:0x64526e21}
I0621 16:36:38.226448   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.40 HWAddress:6e:2f:9c:f4:b6:96 ID:1,6e:2f:9c:f4:b6:96 Lease:0x6450ffe5}
I0621 16:36:38.226461   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.39 HWAddress:72:12:77:32:47:49 ID:1,72:12:77:32:47:49 Lease:0x64523e03}
I0621 16:36:38.226475   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.38 HWAddress:d2:25:e3:2d:31:49 ID:1,d2:25:e3:2d:31:49 Lease:0x6451b25b}
I0621 16:36:38.226488   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.37 HWAddress:5e:a9:62:e6:82:b3 ID:1,5e:a9:62:e6:82:b3 Lease:0x64517e96}
I0621 16:36:38.226508   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.36 HWAddress:9a:5c:d0:fc:fb:3e ID:1,9a:5c:d0:fc:fb:3e Lease:0x64500013}
I0621 16:36:38.226522   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.35 HWAddress:ba:c1:75:8c:94:13 ID:1,ba:c1:75:8c:94:13 Lease:0x644ff9cf}
I0621 16:36:38.226535   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.34 HWAddress:b6:4e:7e:54:f4:e6 ID:1,b6:4e:7e:54:f4:e6 Lease:0x644fb70e}
I0621 16:36:38.226548   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.33 HWAddress:d2:c2:9:16:42:e6 ID:1,d2:c2:9:16:42:e6 Lease:0x644fb655}
I0621 16:36:38.226567   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.32 HWAddress:22:6d:e0:99:d2:87 ID:1,22:6d:e0:99:d2:87 Lease:0x644706a8}
I0621 16:36:38.226580   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.31 HWAddress:ea:5:30:30:d2:15 ID:1,ea:5:30:30:d2:15 Lease:0x6446fd38}
I0621 16:36:38.226593   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.30 HWAddress:82:2e:a7:f9:69:6 ID:1,82:2e:a7:f9:69:6 Lease:0x6446fa8c}
I0621 16:36:38.226606   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.29 HWAddress:4a:ab:16:b1:c9:a9 ID:1,4a:ab:16:b1:c9:a9 Lease:0x6446f6c9}
I0621 16:36:38.226619   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.28 HWAddress:e6:bd:18:1f:e7:4c ID:1,e6:bd:18:1f:e7:4c Lease:0x6446f4d5}
I0621 16:36:38.226633   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.27 HWAddress:ba:22:23:ac:61:85 ID:1,ba:22:23:ac:61:85 Lease:0x6446f41c}
I0621 16:36:38.226646   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.26 HWAddress:2a:8e:ef:28:96:fb ID:1,2a:8e:ef:28:96:fb Lease:0x6446ef65}
I0621 16:36:38.226667   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.25 HWAddress:72:dd:28:42:7b:d3 ID:1,72:dd:28:42:7b:d3 Lease:0x6446eef4}
I0621 16:36:38.226680   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.24 HWAddress:96:89:6b:d4:8:e1 ID:1,96:89:6b:d4:8:e1 Lease:0x6446ee99}
I0621 16:36:38.226694   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.23 HWAddress:4a:e6:7f:50:3f:25 ID:1,4a:e6:7f:50:3f:25 Lease:0x6446ecb5}
I0621 16:36:38.226707   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.22 HWAddress:2e:85:5c:8c:98:7d ID:1,2e:85:5c:8c:98:7d Lease:0x6446ebcf}
I0621 16:36:38.226727   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.21 HWAddress:16:e4:e:cc:b9:83 ID:1,16:e4:e:cc:b9:83 Lease:0x6446e7cb}
I0621 16:36:38.226740   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.20 HWAddress:8a:52:7c:ba:6d:82 ID:1,8a:52:7c:ba:6d:82 Lease:0x644287e4}
I0621 16:36:38.226754   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.19 HWAddress:1e:3c:4e:37:9:ac ID:1,1e:3c:4e:37:9:ac Lease:0x64428247}
I0621 16:36:38.226767   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.18 HWAddress:8e:2c:13:85:8c:a4 ID:1,8e:2c:13:85:8c:a4 Lease:0x64428110}
I0621 16:36:38.226787   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.17 HWAddress:66:53:7:ca:f2:b0 ID:1,66:53:7:ca:f2:b0 Lease:0x64428002}
I0621 16:36:38.226800   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.16 HWAddress:f6:5e:66:47:59:47 ID:1,f6:5e:66:47:59:47 Lease:0x64427baa}
I0621 16:36:38.226813   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.15 HWAddress:4e:63:4b:ba:4f:fb ID:1,4e:63:4b:ba:4f:fb Lease:0x644277cc}
I0621 16:36:38.226826   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.14 HWAddress:92:e8:71:b6:c:e5 ID:1,92:e8:71:b6:c:e5 Lease:0x644270d0}
I0621 16:36:38.226840   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.13 HWAddress:ce:f3:94:6a:24:ce ID:1,ce:f3:94:6a:24:ce Lease:0x644264a6}
I0621 16:36:38.226859   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.12 HWAddress:ca:db:87:22:4:58 ID:1,ca:db:87:22:4:58 Lease:0x644241fe}
I0621 16:36:38.226873   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.11 HWAddress:16:3c:2d:1b:40:d3 ID:1,16:3c:2d:1b:40:d3 Lease:0x64412889}
I0621 16:36:38.226907   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.10 HWAddress:fa:a7:aa:32:8d:43 ID:1,fa:a7:aa:32:8d:43 Lease:0x6441284e}
I0621 16:36:38.227007   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.9 HWAddress:7e:45:19:59:e9:63 ID:1,7e:45:19:59:e9:63 Lease:0x64412806}
I0621 16:36:38.227023   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.8 HWAddress:3a:9d:fb:67:eb:ba ID:1,3a:9d:fb:67:eb:ba Lease:0x64411f96}
I0621 16:36:38.227040   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.7 HWAddress:76:a7:40:98:e9:b7 ID:1,76:a7:40:98:e9:b7 Lease:0x643a7c51}
I0621 16:36:38.227054   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.6 HWAddress:b2:2b:36:17:ec:7a ID:1,b2:2b:36:17:ec:7a Lease:0x642bf388}
I0621 16:36:38.227086   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.5 HWAddress:da:99:b2:56:8d:76 ID:1,da:99:b2:56:8d:76 Lease:0x64297564}
I0621 16:36:38.227100   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.4 HWAddress:16:a2:dc:3e:ee:a3 ID:1,16:a2:dc:3e:ee:a3 Lease:0x6429708c}
I0621 16:36:38.227620   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.3 HWAddress:16:94:64:d9:c7:c8 ID:1,16:94:64:d9:c7:c8 Lease:0x64296d6a}
I0621 16:36:38.227645   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:7a:19:8:2a:35:4c ID:1,7a:19:8:2a:35:4c Lease:0x64284dd1}
I0621 16:36:38.227658   19000 main.go:141] libmachine: dhcp entry: {Name:lima-0 IPAddress:192.168.205.2 HWAddress:52:55:55:63:f9:7d ID:1,52:55:55:63:f9:7d Lease:0x64514587}
I0621 16:36:40.230206   19000 main.go:141] libmachine: Attempt 2
I0621 16:36:40.230233   19000 main.go:141] libmachine: Searching for d2:6e:28:d7:5e:56 in /var/db/dhcpd_leases ...
I0621 16:36:40.230634   19000 main.go:141] libmachine: Found 53 entries in /var/db/dhcpd_leases!
I0621 16:36:40.230656   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.53 HWAddress:86:a2:b9:b:1b:95 ID:1,86:a2:b9:b:1b:95 Lease:0x6493022a}
I0621 16:36:40.230670   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.52 HWAddress:ba:94:7c:fc:ef:eb ID:1,ba:94:7c:fc:ef:eb Lease:0x6492fcfe}
I0621 16:36:40.230684   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.51 HWAddress:ae:12:df:b5:f5:83 ID:1,ae:12:df:b5:f5:83 Lease:0x648da176}
I0621 16:36:40.230697   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.50 HWAddress:a2:e3:b4:1:f4:e7 ID:1,a2:e3:b4:1:f4:e7 Lease:0x6482e5e0}
I0621 16:36:40.230765   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.49 HWAddress:4e:11:49:3f:6b:b4 ID:1,4e:11:49:3f:6b:b4 Lease:0x647f52e2}
I0621 16:36:40.230780   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.48 HWAddress:96:d4:55:90:d9:c2 ID:1,96:d4:55:90:d9:c2 Lease:0x647850dd}
I0621 16:36:40.230793   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.47 HWAddress:ea:52:59:d8:7:5 ID:1,ea:52:59:d8:7:5 Lease:0x6467a61a}
I0621 16:36:40.230807   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.46 HWAddress:2:b5:af:c4:df:d5 ID:1,2:b5:af:c4:df:d5 Lease:0x64676fda}
I0621 16:36:40.230820   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.45 HWAddress:72:92:af:ec:b3:98 ID:1,72:92:af:ec:b3:98 Lease:0x64676c2c}
I0621 16:36:40.230843   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.44 HWAddress:62:f9:1c:d8:ad:d ID:1,62:f9:1c:d8:ad:d Lease:0x6452a5af}
I0621 16:36:40.230860   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.43 HWAddress:2:e0:c4:f4:b1:6c ID:1,2:e0:c4:f4:b1:6c Lease:0x645298cf}
I0621 16:36:40.230873   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.42 HWAddress:72:9a:65:1d:7:8f ID:1,72:9a:65:1d:7:8f Lease:0x64529727}
I0621 16:36:40.230916   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.41 HWAddress:ae:61:59:6d:f1:c9 ID:1,ae:61:59:6d:f1:c9 Lease:0x64526e21}
I0621 16:36:40.230931   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.40 HWAddress:6e:2f:9c:f4:b6:96 ID:1,6e:2f:9c:f4:b6:96 Lease:0x6450ffe5}
I0621 16:36:40.230945   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.39 HWAddress:72:12:77:32:47:49 ID:1,72:12:77:32:47:49 Lease:0x64523e03}
I0621 16:36:40.230958   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.38 HWAddress:d2:25:e3:2d:31:49 ID:1,d2:25:e3:2d:31:49 Lease:0x6451b25b}
I0621 16:36:40.230971   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.37 HWAddress:5e:a9:62:e6:82:b3 ID:1,5e:a9:62:e6:82:b3 Lease:0x64517e96}
I0621 16:36:40.230984   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.36 HWAddress:9a:5c:d0:fc:fb:3e ID:1,9a:5c:d0:fc:fb:3e Lease:0x64500013}
I0621 16:36:40.230997   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.35 HWAddress:ba:c1:75:8c:94:13 ID:1,ba:c1:75:8c:94:13 Lease:0x644ff9cf}
I0621 16:36:40.231011   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.34 HWAddress:b6:4e:7e:54:f4:e6 ID:1,b6:4e:7e:54:f4:e6 Lease:0x644fb70e}
I0621 16:36:40.231024   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.33 HWAddress:d2:c2:9:16:42:e6 ID:1,d2:c2:9:16:42:e6 Lease:0x644fb655}
I0621 16:36:40.231038   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.32 HWAddress:22:6d:e0:99:d2:87 ID:1,22:6d:e0:99:d2:87 Lease:0x644706a8}
I0621 16:36:40.231053   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.31 HWAddress:ea:5:30:30:d2:15 ID:1,ea:5:30:30:d2:15 Lease:0x6446fd38}
I0621 16:36:40.231065   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.30 HWAddress:82:2e:a7:f9:69:6 ID:1,82:2e:a7:f9:69:6 Lease:0x6446fa8c}
I0621 16:36:40.231080   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.29 HWAddress:4a:ab:16:b1:c9:a9 ID:1,4a:ab:16:b1:c9:a9 Lease:0x6446f6c9}
I0621 16:36:40.231093   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.28 HWAddress:e6:bd:18:1f:e7:4c ID:1,e6:bd:18:1f:e7:4c Lease:0x6446f4d5}
I0621 16:36:40.231112   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.27 HWAddress:ba:22:23:ac:61:85 ID:1,ba:22:23:ac:61:85 Lease:0x6446f41c}
I0621 16:36:40.231125   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.26 HWAddress:2a:8e:ef:28:96:fb ID:1,2a:8e:ef:28:96:fb Lease:0x6446ef65}
I0621 16:36:40.231138   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.25 HWAddress:72:dd:28:42:7b:d3 ID:1,72:dd:28:42:7b:d3 Lease:0x6446eef4}
I0621 16:36:40.231152   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.24 HWAddress:96:89:6b:d4:8:e1 ID:1,96:89:6b:d4:8:e1 Lease:0x6446ee99}
I0621 16:36:40.231166   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.23 HWAddress:4a:e6:7f:50:3f:25 ID:1,4a:e6:7f:50:3f:25 Lease:0x6446ecb5}
I0621 16:36:40.231179   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.22 HWAddress:2e:85:5c:8c:98:7d ID:1,2e:85:5c:8c:98:7d Lease:0x6446ebcf}
I0621 16:36:40.231193   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.21 HWAddress:16:e4:e:cc:b9:83 ID:1,16:e4:e:cc:b9:83 Lease:0x6446e7cb}
I0621 16:36:40.231206   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.20 HWAddress:8a:52:7c:ba:6d:82 ID:1,8a:52:7c:ba:6d:82 Lease:0x644287e4}
I0621 16:36:40.231219   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.19 HWAddress:1e:3c:4e:37:9:ac ID:1,1e:3c:4e:37:9:ac Lease:0x64428247}
I0621 16:36:40.231232   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.18 HWAddress:8e:2c:13:85:8c:a4 ID:1,8e:2c:13:85:8c:a4 Lease:0x64428110}
I0621 16:36:40.231246   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.17 HWAddress:66:53:7:ca:f2:b0 ID:1,66:53:7:ca:f2:b0 Lease:0x64428002}
I0621 16:36:40.231259   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.16 HWAddress:f6:5e:66:47:59:47 ID:1,f6:5e:66:47:59:47 Lease:0x64427baa}
I0621 16:36:40.231272   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.15 HWAddress:4e:63:4b:ba:4f:fb ID:1,4e:63:4b:ba:4f:fb Lease:0x644277cc}
I0621 16:36:40.231285   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.14 HWAddress:92:e8:71:b6:c:e5 ID:1,92:e8:71:b6:c:e5 Lease:0x644270d0}
I0621 16:36:40.231298   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.13 HWAddress:ce:f3:94:6a:24:ce ID:1,ce:f3:94:6a:24:ce Lease:0x644264a6}
I0621 16:36:40.231311   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.12 HWAddress:ca:db:87:22:4:58 ID:1,ca:db:87:22:4:58 Lease:0x644241fe}
I0621 16:36:40.231324   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.11 HWAddress:16:3c:2d:1b:40:d3 ID:1,16:3c:2d:1b:40:d3 Lease:0x64412889}
I0621 16:36:40.231337   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.10 HWAddress:fa:a7:aa:32:8d:43 ID:1,fa:a7:aa:32:8d:43 Lease:0x6441284e}
I0621 16:36:40.231350   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.9 HWAddress:7e:45:19:59:e9:63 ID:1,7e:45:19:59:e9:63 Lease:0x64412806}
I0621 16:36:40.231363   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.8 HWAddress:3a:9d:fb:67:eb:ba ID:1,3a:9d:fb:67:eb:ba Lease:0x64411f96}
I0621 16:36:40.231376   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.7 HWAddress:76:a7:40:98:e9:b7 ID:1,76:a7:40:98:e9:b7 Lease:0x643a7c51}
I0621 16:36:40.231389   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.6 HWAddress:b2:2b:36:17:ec:7a ID:1,b2:2b:36:17:ec:7a Lease:0x642bf388}
I0621 16:36:40.231403   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.5 HWAddress:da:99:b2:56:8d:76 ID:1,da:99:b2:56:8d:76 Lease:0x64297564}
I0621 16:36:40.231416   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.4 HWAddress:16:a2:dc:3e:ee:a3 ID:1,16:a2:dc:3e:ee:a3 Lease:0x6429708c}
I0621 16:36:40.231429   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.3 HWAddress:16:94:64:d9:c7:c8 ID:1,16:94:64:d9:c7:c8 Lease:0x64296d6a}
I0621 16:36:40.231443   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:7a:19:8:2a:35:4c ID:1,7a:19:8:2a:35:4c Lease:0x64284dd1}
I0621 16:36:40.231456   19000 main.go:141] libmachine: dhcp entry: {Name:lima-0 IPAddress:192.168.205.2 HWAddress:52:55:55:63:f9:7d ID:1,52:55:55:63:f9:7d Lease:0x64514587}
I0621 16:36:42.232588   19000 main.go:141] libmachine: Attempt 3
I0621 16:36:42.232597   19000 main.go:141] libmachine: Searching for d2:6e:28:d7:5e:56 in /var/db/dhcpd_leases ...
I0621 16:36:42.232791   19000 main.go:141] libmachine: Found 53 entries in /var/db/dhcpd_leases!
I0621 16:36:42.232803   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.53 HWAddress:86:a2:b9:b:1b:95 ID:1,86:a2:b9:b:1b:95 Lease:0x6493022a}
I0621 16:36:42.232807   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.52 HWAddress:ba:94:7c:fc:ef:eb ID:1,ba:94:7c:fc:ef:eb Lease:0x6492fcfe}
I0621 16:36:42.232812   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.51 HWAddress:ae:12:df:b5:f5:83 ID:1,ae:12:df:b5:f5:83 Lease:0x648da176}
I0621 16:36:42.232819   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.50 HWAddress:a2:e3:b4:1:f4:e7 ID:1,a2:e3:b4:1:f4:e7 Lease:0x6482e5e0}
I0621 16:36:42.232824   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.49 HWAddress:4e:11:49:3f:6b:b4 ID:1,4e:11:49:3f:6b:b4 Lease:0x647f52e2}
I0621 16:36:42.232828   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.48 HWAddress:96:d4:55:90:d9:c2 ID:1,96:d4:55:90:d9:c2 Lease:0x647850dd}
I0621 16:36:42.232833   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.47 HWAddress:ea:52:59:d8:7:5 ID:1,ea:52:59:d8:7:5 Lease:0x6467a61a}
I0621 16:36:42.232838   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.46 HWAddress:2:b5:af:c4:df:d5 ID:1,2:b5:af:c4:df:d5 Lease:0x64676fda}
I0621 16:36:42.232842   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.45 HWAddress:72:92:af:ec:b3:98 ID:1,72:92:af:ec:b3:98 Lease:0x64676c2c}
I0621 16:36:42.232847   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.44 HWAddress:62:f9:1c:d8:ad:d ID:1,62:f9:1c:d8:ad:d Lease:0x6452a5af}
I0621 16:36:42.232851   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.43 HWAddress:2:e0:c4:f4:b1:6c ID:1,2:e0:c4:f4:b1:6c Lease:0x645298cf}
I0621 16:36:42.232858   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.42 HWAddress:72:9a:65:1d:7:8f ID:1,72:9a:65:1d:7:8f Lease:0x64529727}
I0621 16:36:42.232862   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.41 HWAddress:ae:61:59:6d:f1:c9 ID:1,ae:61:59:6d:f1:c9 Lease:0x64526e21}
I0621 16:36:42.232867   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.40 HWAddress:6e:2f:9c:f4:b6:96 ID:1,6e:2f:9c:f4:b6:96 Lease:0x6450ffe5}
I0621 16:36:42.232871   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.39 HWAddress:72:12:77:32:47:49 ID:1,72:12:77:32:47:49 Lease:0x64523e03}
I0621 16:36:42.232875   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.38 HWAddress:d2:25:e3:2d:31:49 ID:1,d2:25:e3:2d:31:49 Lease:0x6451b25b}
I0621 16:36:42.232880   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.37 HWAddress:5e:a9:62:e6:82:b3 ID:1,5e:a9:62:e6:82:b3 Lease:0x64517e96}
I0621 16:36:42.232884   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.36 HWAddress:9a:5c:d0:fc:fb:3e ID:1,9a:5c:d0:fc:fb:3e Lease:0x64500013}
I0621 16:36:42.232889   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.35 HWAddress:ba:c1:75:8c:94:13 ID:1,ba:c1:75:8c:94:13 Lease:0x644ff9cf}
I0621 16:36:42.232893   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.34 HWAddress:b6:4e:7e:54:f4:e6 ID:1,b6:4e:7e:54:f4:e6 Lease:0x644fb70e}
I0621 16:36:42.232900   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.33 HWAddress:d2:c2:9:16:42:e6 ID:1,d2:c2:9:16:42:e6 Lease:0x644fb655}
I0621 16:36:42.232905   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.32 HWAddress:22:6d:e0:99:d2:87 ID:1,22:6d:e0:99:d2:87 Lease:0x644706a8}
I0621 16:36:42.232909   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.31 HWAddress:ea:5:30:30:d2:15 ID:1,ea:5:30:30:d2:15 Lease:0x6446fd38}
I0621 16:36:42.232914   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.30 HWAddress:82:2e:a7:f9:69:6 ID:1,82:2e:a7:f9:69:6 Lease:0x6446fa8c}
I0621 16:36:42.232918   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.29 HWAddress:4a:ab:16:b1:c9:a9 ID:1,4a:ab:16:b1:c9:a9 Lease:0x6446f6c9}
I0621 16:36:42.232923   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.28 HWAddress:e6:bd:18:1f:e7:4c ID:1,e6:bd:18:1f:e7:4c Lease:0x6446f4d5}
I0621 16:36:42.232927   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.27 HWAddress:ba:22:23:ac:61:85 ID:1,ba:22:23:ac:61:85 Lease:0x6446f41c}
I0621 16:36:42.232932   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.26 HWAddress:2a:8e:ef:28:96:fb ID:1,2a:8e:ef:28:96:fb Lease:0x6446ef65}
I0621 16:36:42.232936   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.25 HWAddress:72:dd:28:42:7b:d3 ID:1,72:dd:28:42:7b:d3 Lease:0x6446eef4}
I0621 16:36:42.232940   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.24 HWAddress:96:89:6b:d4:8:e1 ID:1,96:89:6b:d4:8:e1 Lease:0x6446ee99}
I0621 16:36:42.232945   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.23 HWAddress:4a:e6:7f:50:3f:25 ID:1,4a:e6:7f:50:3f:25 Lease:0x6446ecb5}
I0621 16:36:42.232949   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.22 HWAddress:2e:85:5c:8c:98:7d ID:1,2e:85:5c:8c:98:7d Lease:0x6446ebcf}
I0621 16:36:42.232954   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.21 HWAddress:16:e4:e:cc:b9:83 ID:1,16:e4:e:cc:b9:83 Lease:0x6446e7cb}
I0621 16:36:42.232958   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.20 HWAddress:8a:52:7c:ba:6d:82 ID:1,8a:52:7c:ba:6d:82 Lease:0x644287e4}
I0621 16:36:42.232963   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.19 HWAddress:1e:3c:4e:37:9:ac ID:1,1e:3c:4e:37:9:ac Lease:0x64428247}
I0621 16:36:42.232967   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.18 HWAddress:8e:2c:13:85:8c:a4 ID:1,8e:2c:13:85:8c:a4 Lease:0x64428110}
I0621 16:36:42.232972   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.17 HWAddress:66:53:7:ca:f2:b0 ID:1,66:53:7:ca:f2:b0 Lease:0x64428002}
I0621 16:36:42.232976   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.16 HWAddress:f6:5e:66:47:59:47 ID:1,f6:5e:66:47:59:47 Lease:0x64427baa}
I0621 16:36:42.232982   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.15 HWAddress:4e:63:4b:ba:4f:fb ID:1,4e:63:4b:ba:4f:fb Lease:0x644277cc}
I0621 16:36:42.232986   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.14 HWAddress:92:e8:71:b6:c:e5 ID:1,92:e8:71:b6:c:e5 Lease:0x644270d0}
I0621 16:36:42.232991   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.13 HWAddress:ce:f3:94:6a:24:ce ID:1,ce:f3:94:6a:24:ce Lease:0x644264a6}
I0621 16:36:42.232995   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.12 HWAddress:ca:db:87:22:4:58 ID:1,ca:db:87:22:4:58 Lease:0x644241fe}
I0621 16:36:42.233002   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.11 HWAddress:16:3c:2d:1b:40:d3 ID:1,16:3c:2d:1b:40:d3 Lease:0x64412889}
I0621 16:36:42.233007   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.10 HWAddress:fa:a7:aa:32:8d:43 ID:1,fa:a7:aa:32:8d:43 Lease:0x6441284e}
I0621 16:36:42.233011   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.9 HWAddress:7e:45:19:59:e9:63 ID:1,7e:45:19:59:e9:63 Lease:0x64412806}
I0621 16:36:42.233016   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.8 HWAddress:3a:9d:fb:67:eb:ba ID:1,3a:9d:fb:67:eb:ba Lease:0x64411f96}
I0621 16:36:42.233020   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.7 HWAddress:76:a7:40:98:e9:b7 ID:1,76:a7:40:98:e9:b7 Lease:0x643a7c51}
I0621 16:36:42.233024   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.6 HWAddress:b2:2b:36:17:ec:7a ID:1,b2:2b:36:17:ec:7a Lease:0x642bf388}
I0621 16:36:42.233029   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.5 HWAddress:da:99:b2:56:8d:76 ID:1,da:99:b2:56:8d:76 Lease:0x64297564}
I0621 16:36:42.233033   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.4 HWAddress:16:a2:dc:3e:ee:a3 ID:1,16:a2:dc:3e:ee:a3 Lease:0x6429708c}
I0621 16:36:42.233038   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.3 HWAddress:16:94:64:d9:c7:c8 ID:1,16:94:64:d9:c7:c8 Lease:0x64296d6a}
I0621 16:36:42.233042   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:7a:19:8:2a:35:4c ID:1,7a:19:8:2a:35:4c Lease:0x64284dd1}
I0621 16:36:42.233047   19000 main.go:141] libmachine: dhcp entry: {Name:lima-0 IPAddress:192.168.205.2 HWAddress:52:55:55:63:f9:7d ID:1,52:55:55:63:f9:7d Lease:0x64514587}
I0621 16:36:44.234177   19000 main.go:141] libmachine: Attempt 4
I0621 16:36:44.234187   19000 main.go:141] libmachine: Searching for d2:6e:28:d7:5e:56 in /var/db/dhcpd_leases ...
I0621 16:36:44.234356   19000 main.go:141] libmachine: Found 53 entries in /var/db/dhcpd_leases!
I0621 16:36:44.234365   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.53 HWAddress:86:a2:b9:b:1b:95 ID:1,86:a2:b9:b:1b:95 Lease:0x6493022a}
I0621 16:36:44.234370   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.52 HWAddress:ba:94:7c:fc:ef:eb ID:1,ba:94:7c:fc:ef:eb Lease:0x6492fcfe}
I0621 16:36:44.235291   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.51 HWAddress:ae:12:df:b5:f5:83 ID:1,ae:12:df:b5:f5:83 Lease:0x648da176}
I0621 16:36:44.235296   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.50 HWAddress:a2:e3:b4:1:f4:e7 ID:1,a2:e3:b4:1:f4:e7 Lease:0x6482e5e0}
I0621 16:36:44.235306   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.49 HWAddress:4e:11:49:3f:6b:b4 ID:1,4e:11:49:3f:6b:b4 Lease:0x647f52e2}
I0621 16:36:44.235311   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.48 HWAddress:96:d4:55:90:d9:c2 ID:1,96:d4:55:90:d9:c2 Lease:0x647850dd}
I0621 16:36:44.235315   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.47 HWAddress:ea:52:59:d8:7:5 ID:1,ea:52:59:d8:7:5 Lease:0x6467a61a}
I0621 16:36:44.235320   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.46 HWAddress:2:b5:af:c4:df:d5 ID:1,2:b5:af:c4:df:d5 Lease:0x64676fda}
I0621 16:36:44.235327   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.45 HWAddress:72:92:af:ec:b3:98 ID:1,72:92:af:ec:b3:98 Lease:0x64676c2c}
I0621 16:36:44.235332   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.44 HWAddress:62:f9:1c:d8:ad:d ID:1,62:f9:1c:d8:ad:d Lease:0x6452a5af}
I0621 16:36:44.235336   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.43 HWAddress:2:e0:c4:f4:b1:6c ID:1,2:e0:c4:f4:b1:6c Lease:0x645298cf}
I0621 16:36:44.235344   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.42 HWAddress:72:9a:65:1d:7:8f ID:1,72:9a:65:1d:7:8f Lease:0x64529727}
I0621 16:36:44.235348   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.41 HWAddress:ae:61:59:6d:f1:c9 ID:1,ae:61:59:6d:f1:c9 Lease:0x64526e21}
I0621 16:36:44.235353   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.40 HWAddress:6e:2f:9c:f4:b6:96 ID:1,6e:2f:9c:f4:b6:96 Lease:0x6450ffe5}
I0621 16:36:44.235357   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.39 HWAddress:72:12:77:32:47:49 ID:1,72:12:77:32:47:49 Lease:0x64523e03}
I0621 16:36:44.235362   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.38 HWAddress:d2:25:e3:2d:31:49 ID:1,d2:25:e3:2d:31:49 Lease:0x6451b25b}
I0621 16:36:44.235367   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.37 HWAddress:5e:a9:62:e6:82:b3 ID:1,5e:a9:62:e6:82:b3 Lease:0x64517e96}
I0621 16:36:44.235371   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.36 HWAddress:9a:5c:d0:fc:fb:3e ID:1,9a:5c:d0:fc:fb:3e Lease:0x64500013}
I0621 16:36:44.235376   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.35 HWAddress:ba:c1:75:8c:94:13 ID:1,ba:c1:75:8c:94:13 Lease:0x644ff9cf}
I0621 16:36:44.235380   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.34 HWAddress:b6:4e:7e:54:f4:e6 ID:1,b6:4e:7e:54:f4:e6 Lease:0x644fb70e}
I0621 16:36:44.235384   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.33 HWAddress:d2:c2:9:16:42:e6 ID:1,d2:c2:9:16:42:e6 Lease:0x644fb655}
I0621 16:36:44.235390   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.32 HWAddress:22:6d:e0:99:d2:87 ID:1,22:6d:e0:99:d2:87 Lease:0x644706a8}
I0621 16:36:44.235394   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.31 HWAddress:ea:5:30:30:d2:15 ID:1,ea:5:30:30:d2:15 Lease:0x6446fd38}
I0621 16:36:44.235398   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.30 HWAddress:82:2e:a7:f9:69:6 ID:1,82:2e:a7:f9:69:6 Lease:0x6446fa8c}
I0621 16:36:44.235403   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.29 HWAddress:4a:ab:16:b1:c9:a9 ID:1,4a:ab:16:b1:c9:a9 Lease:0x6446f6c9}
I0621 16:36:44.235407   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.28 HWAddress:e6:bd:18:1f:e7:4c ID:1,e6:bd:18:1f:e7:4c Lease:0x6446f4d5}
I0621 16:36:44.235412   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.27 HWAddress:ba:22:23:ac:61:85 ID:1,ba:22:23:ac:61:85 Lease:0x6446f41c}
I0621 16:36:44.235416   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.26 HWAddress:2a:8e:ef:28:96:fb ID:1,2a:8e:ef:28:96:fb Lease:0x6446ef65}
I0621 16:36:44.235420   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.25 HWAddress:72:dd:28:42:7b:d3 ID:1,72:dd:28:42:7b:d3 Lease:0x6446eef4}
I0621 16:36:44.235425   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.24 HWAddress:96:89:6b:d4:8:e1 ID:1,96:89:6b:d4:8:e1 Lease:0x6446ee99}
I0621 16:36:44.235429   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.23 HWAddress:4a:e6:7f:50:3f:25 ID:1,4a:e6:7f:50:3f:25 Lease:0x6446ecb5}
I0621 16:36:44.235434   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.22 HWAddress:2e:85:5c:8c:98:7d ID:1,2e:85:5c:8c:98:7d Lease:0x6446ebcf}
I0621 16:36:44.235438   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.21 HWAddress:16:e4:e:cc:b9:83 ID:1,16:e4:e:cc:b9:83 Lease:0x6446e7cb}
I0621 16:36:44.235442   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.20 HWAddress:8a:52:7c:ba:6d:82 ID:1,8a:52:7c:ba:6d:82 Lease:0x644287e4}
I0621 16:36:44.235446   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.19 HWAddress:1e:3c:4e:37:9:ac ID:1,1e:3c:4e:37:9:ac Lease:0x64428247}
I0621 16:36:44.235451   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.18 HWAddress:8e:2c:13:85:8c:a4 ID:1,8e:2c:13:85:8c:a4 Lease:0x64428110}
I0621 16:36:44.235455   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.17 HWAddress:66:53:7:ca:f2:b0 ID:1,66:53:7:ca:f2:b0 Lease:0x64428002}
I0621 16:36:44.235460   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.16 HWAddress:f6:5e:66:47:59:47 ID:1,f6:5e:66:47:59:47 Lease:0x64427baa}
I0621 16:36:44.235464   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.15 HWAddress:4e:63:4b:ba:4f:fb ID:1,4e:63:4b:ba:4f:fb Lease:0x644277cc}
I0621 16:36:44.235468   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.14 HWAddress:92:e8:71:b6:c:e5 ID:1,92:e8:71:b6:c:e5 Lease:0x644270d0}
I0621 16:36:44.235473   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.13 HWAddress:ce:f3:94:6a:24:ce ID:1,ce:f3:94:6a:24:ce Lease:0x644264a6}
I0621 16:36:44.235477   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.12 HWAddress:ca:db:87:22:4:58 ID:1,ca:db:87:22:4:58 Lease:0x644241fe}
I0621 16:36:44.235481   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.11 HWAddress:16:3c:2d:1b:40:d3 ID:1,16:3c:2d:1b:40:d3 Lease:0x64412889}
I0621 16:36:44.235486   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.10 HWAddress:fa:a7:aa:32:8d:43 ID:1,fa:a7:aa:32:8d:43 Lease:0x6441284e}
I0621 16:36:44.235490   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.9 HWAddress:7e:45:19:59:e9:63 ID:1,7e:45:19:59:e9:63 Lease:0x64412806}
I0621 16:36:44.235494   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.8 HWAddress:3a:9d:fb:67:eb:ba ID:1,3a:9d:fb:67:eb:ba Lease:0x64411f96}
I0621 16:36:44.235499   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.7 HWAddress:76:a7:40:98:e9:b7 ID:1,76:a7:40:98:e9:b7 Lease:0x643a7c51}
I0621 16:36:44.235503   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.6 HWAddress:b2:2b:36:17:ec:7a ID:1,b2:2b:36:17:ec:7a Lease:0x642bf388}
I0621 16:36:44.235508   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.5 HWAddress:da:99:b2:56:8d:76 ID:1,da:99:b2:56:8d:76 Lease:0x64297564}
I0621 16:36:44.235515   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.4 HWAddress:16:a2:dc:3e:ee:a3 ID:1,16:a2:dc:3e:ee:a3 Lease:0x6429708c}
I0621 16:36:44.235519   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.3 HWAddress:16:94:64:d9:c7:c8 ID:1,16:94:64:d9:c7:c8 Lease:0x64296d6a}
I0621 16:36:44.235524   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:7a:19:8:2a:35:4c ID:1,7a:19:8:2a:35:4c Lease:0x64284dd1}
I0621 16:36:44.235528   19000 main.go:141] libmachine: dhcp entry: {Name:lima-0 IPAddress:192.168.205.2 HWAddress:52:55:55:63:f9:7d ID:1,52:55:55:63:f9:7d Lease:0x64514587}
I0621 16:36:46.235920   19000 main.go:141] libmachine: Attempt 5
I0621 16:36:46.235932   19000 main.go:141] libmachine: Searching for d2:6e:28:d7:5e:56 in /var/db/dhcpd_leases ...
I0621 16:36:46.236082   19000 main.go:141] libmachine: Found 53 entries in /var/db/dhcpd_leases!
I0621 16:36:46.236091   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.53 HWAddress:86:a2:b9:b:1b:95 ID:1,86:a2:b9:b:1b:95 Lease:0x6493022a}
I0621 16:36:46.236100   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.52 HWAddress:ba:94:7c:fc:ef:eb ID:1,ba:94:7c:fc:ef:eb Lease:0x6492fcfe}
I0621 16:36:46.236105   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.51 HWAddress:ae:12:df:b5:f5:83 ID:1,ae:12:df:b5:f5:83 Lease:0x648da176}
I0621 16:36:46.236110   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.50 HWAddress:a2:e3:b4:1:f4:e7 ID:1,a2:e3:b4:1:f4:e7 Lease:0x6482e5e0}
I0621 16:36:46.236114   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.49 HWAddress:4e:11:49:3f:6b:b4 ID:1,4e:11:49:3f:6b:b4 Lease:0x647f52e2}
I0621 16:36:46.236119   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.48 HWAddress:96:d4:55:90:d9:c2 ID:1,96:d4:55:90:d9:c2 Lease:0x647850dd}
I0621 16:36:46.236123   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.47 HWAddress:ea:52:59:d8:7:5 ID:1,ea:52:59:d8:7:5 Lease:0x6467a61a}
I0621 16:36:46.236128   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.46 HWAddress:2:b5:af:c4:df:d5 ID:1,2:b5:af:c4:df:d5 Lease:0x64676fda}
I0621 16:36:46.236132   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.45 HWAddress:72:92:af:ec:b3:98 ID:1,72:92:af:ec:b3:98 Lease:0x64676c2c}
I0621 16:36:46.236137   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.44 HWAddress:62:f9:1c:d8:ad:d ID:1,62:f9:1c:d8:ad:d Lease:0x6452a5af}
I0621 16:36:46.236141   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.43 HWAddress:2:e0:c4:f4:b1:6c ID:1,2:e0:c4:f4:b1:6c Lease:0x645298cf}
I0621 16:36:46.236146   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.42 HWAddress:72:9a:65:1d:7:8f ID:1,72:9a:65:1d:7:8f Lease:0x64529727}
I0621 16:36:46.236150   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.41 HWAddress:ae:61:59:6d:f1:c9 ID:1,ae:61:59:6d:f1:c9 Lease:0x64526e21}
I0621 16:36:46.236155   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.40 HWAddress:6e:2f:9c:f4:b6:96 ID:1,6e:2f:9c:f4:b6:96 Lease:0x6450ffe5}
I0621 16:36:46.236159   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.39 HWAddress:72:12:77:32:47:49 ID:1,72:12:77:32:47:49 Lease:0x64523e03}
I0621 16:36:46.236164   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.38 HWAddress:d2:25:e3:2d:31:49 ID:1,d2:25:e3:2d:31:49 Lease:0x6451b25b}
I0621 16:36:46.236168   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.37 HWAddress:5e:a9:62:e6:82:b3 ID:1,5e:a9:62:e6:82:b3 Lease:0x64517e96}
I0621 16:36:46.236172   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.36 HWAddress:9a:5c:d0:fc:fb:3e ID:1,9a:5c:d0:fc:fb:3e Lease:0x64500013}
I0621 16:36:46.236177   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.35 HWAddress:ba:c1:75:8c:94:13 ID:1,ba:c1:75:8c:94:13 Lease:0x644ff9cf}
I0621 16:36:46.236184   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.34 HWAddress:b6:4e:7e:54:f4:e6 ID:1,b6:4e:7e:54:f4:e6 Lease:0x644fb70e}
I0621 16:36:46.236188   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.33 HWAddress:d2:c2:9:16:42:e6 ID:1,d2:c2:9:16:42:e6 Lease:0x644fb655}
I0621 16:36:46.236192   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.32 HWAddress:22:6d:e0:99:d2:87 ID:1,22:6d:e0:99:d2:87 Lease:0x644706a8}
I0621 16:36:46.236198   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.31 HWAddress:ea:5:30:30:d2:15 ID:1,ea:5:30:30:d2:15 Lease:0x6446fd38}
I0621 16:36:46.236202   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.30 HWAddress:82:2e:a7:f9:69:6 ID:1,82:2e:a7:f9:69:6 Lease:0x6446fa8c}
I0621 16:36:46.236207   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.29 HWAddress:4a:ab:16:b1:c9:a9 ID:1,4a:ab:16:b1:c9:a9 Lease:0x6446f6c9}
I0621 16:36:46.236211   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.28 HWAddress:e6:bd:18:1f:e7:4c ID:1,e6:bd:18:1f:e7:4c Lease:0x6446f4d5}
I0621 16:36:46.236215   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.27 HWAddress:ba:22:23:ac:61:85 ID:1,ba:22:23:ac:61:85 Lease:0x6446f41c}
I0621 16:36:46.236220   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.26 HWAddress:2a:8e:ef:28:96:fb ID:1,2a:8e:ef:28:96:fb Lease:0x6446ef65}
I0621 16:36:46.236224   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.25 HWAddress:72:dd:28:42:7b:d3 ID:1,72:dd:28:42:7b:d3 Lease:0x6446eef4}
I0621 16:36:46.236229   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.24 HWAddress:96:89:6b:d4:8:e1 ID:1,96:89:6b:d4:8:e1 Lease:0x6446ee99}
I0621 16:36:46.236233   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.23 HWAddress:4a:e6:7f:50:3f:25 ID:1,4a:e6:7f:50:3f:25 Lease:0x6446ecb5}
I0621 16:36:46.236238   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.22 HWAddress:2e:85:5c:8c:98:7d ID:1,2e:85:5c:8c:98:7d Lease:0x6446ebcf}
I0621 16:36:46.236242   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.21 HWAddress:16:e4:e:cc:b9:83 ID:1,16:e4:e:cc:b9:83 Lease:0x6446e7cb}
I0621 16:36:46.236246   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.20 HWAddress:8a:52:7c:ba:6d:82 ID:1,8a:52:7c:ba:6d:82 Lease:0x644287e4}
I0621 16:36:46.236251   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.19 HWAddress:1e:3c:4e:37:9:ac ID:1,1e:3c:4e:37:9:ac Lease:0x64428247}
I0621 16:36:46.236255   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.18 HWAddress:8e:2c:13:85:8c:a4 ID:1,8e:2c:13:85:8c:a4 Lease:0x64428110}
I0621 16:36:46.236260   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.17 HWAddress:66:53:7:ca:f2:b0 ID:1,66:53:7:ca:f2:b0 Lease:0x64428002}
I0621 16:36:46.236268   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.16 HWAddress:f6:5e:66:47:59:47 ID:1,f6:5e:66:47:59:47 Lease:0x64427baa}
I0621 16:36:46.236272   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.15 HWAddress:4e:63:4b:ba:4f:fb ID:1,4e:63:4b:ba:4f:fb Lease:0x644277cc}
I0621 16:36:46.236277   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.14 HWAddress:92:e8:71:b6:c:e5 ID:1,92:e8:71:b6:c:e5 Lease:0x644270d0}
I0621 16:36:46.236281   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.13 HWAddress:ce:f3:94:6a:24:ce ID:1,ce:f3:94:6a:24:ce Lease:0x644264a6}
I0621 16:36:46.236285   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.12 HWAddress:ca:db:87:22:4:58 ID:1,ca:db:87:22:4:58 Lease:0x644241fe}
I0621 16:36:46.236290   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.11 HWAddress:16:3c:2d:1b:40:d3 ID:1,16:3c:2d:1b:40:d3 Lease:0x64412889}
I0621 16:36:46.236294   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.10 HWAddress:fa:a7:aa:32:8d:43 ID:1,fa:a7:aa:32:8d:43 Lease:0x6441284e}
I0621 16:36:46.236299   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.9 HWAddress:7e:45:19:59:e9:63 ID:1,7e:45:19:59:e9:63 Lease:0x64412806}
I0621 16:36:46.236304   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.8 HWAddress:3a:9d:fb:67:eb:ba ID:1,3a:9d:fb:67:eb:ba Lease:0x64411f96}
I0621 16:36:46.236308   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.7 HWAddress:76:a7:40:98:e9:b7 ID:1,76:a7:40:98:e9:b7 Lease:0x643a7c51}
I0621 16:36:46.236313   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.6 HWAddress:b2:2b:36:17:ec:7a ID:1,b2:2b:36:17:ec:7a Lease:0x642bf388}
I0621 16:36:46.236329   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.5 HWAddress:da:99:b2:56:8d:76 ID:1,da:99:b2:56:8d:76 Lease:0x64297564}
I0621 16:36:46.236334   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.4 HWAddress:16:a2:dc:3e:ee:a3 ID:1,16:a2:dc:3e:ee:a3 Lease:0x6429708c}
I0621 16:36:46.236338   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.3 HWAddress:16:94:64:d9:c7:c8 ID:1,16:94:64:d9:c7:c8 Lease:0x64296d6a}
I0621 16:36:46.236343   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:7a:19:8:2a:35:4c ID:1,7a:19:8:2a:35:4c Lease:0x64284dd1}
I0621 16:36:46.236347   19000 main.go:141] libmachine: dhcp entry: {Name:lima-0 IPAddress:192.168.205.2 HWAddress:52:55:55:63:f9:7d ID:1,52:55:55:63:f9:7d Lease:0x64514587}
I0621 16:36:48.236775   19000 main.go:141] libmachine: Attempt 6
I0621 16:36:48.236787   19000 main.go:141] libmachine: Searching for d2:6e:28:d7:5e:56 in /var/db/dhcpd_leases ...
I0621 16:36:48.236992   19000 main.go:141] libmachine: Found 53 entries in /var/db/dhcpd_leases!
I0621 16:36:48.237001   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.53 HWAddress:86:a2:b9:b:1b:95 ID:1,86:a2:b9:b:1b:95 Lease:0x6493022a}
I0621 16:36:48.237006   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.52 HWAddress:ba:94:7c:fc:ef:eb ID:1,ba:94:7c:fc:ef:eb Lease:0x6492fcfe}
I0621 16:36:48.237011   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.51 HWAddress:ae:12:df:b5:f5:83 ID:1,ae:12:df:b5:f5:83 Lease:0x648da176}
I0621 16:36:48.237015   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.50 HWAddress:a2:e3:b4:1:f4:e7 ID:1,a2:e3:b4:1:f4:e7 Lease:0x6482e5e0}
I0621 16:36:48.237020   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.49 HWAddress:4e:11:49:3f:6b:b4 ID:1,4e:11:49:3f:6b:b4 Lease:0x647f52e2}
I0621 16:36:48.237024   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.48 HWAddress:96:d4:55:90:d9:c2 ID:1,96:d4:55:90:d9:c2 Lease:0x647850dd}
I0621 16:36:48.237028   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.47 HWAddress:ea:52:59:d8:7:5 ID:1,ea:52:59:d8:7:5 Lease:0x6467a61a}
I0621 16:36:48.237033   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.46 HWAddress:2:b5:af:c4:df:d5 ID:1,2:b5:af:c4:df:d5 Lease:0x64676fda}
I0621 16:36:48.237037   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.45 HWAddress:72:92:af:ec:b3:98 ID:1,72:92:af:ec:b3:98 Lease:0x64676c2c}
I0621 16:36:48.237042   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.44 HWAddress:62:f9:1c:d8:ad:d ID:1,62:f9:1c:d8:ad:d Lease:0x6452a5af}
I0621 16:36:48.237046   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.43 HWAddress:2:e0:c4:f4:b1:6c ID:1,2:e0:c4:f4:b1:6c Lease:0x645298cf}
I0621 16:36:48.237050   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.42 HWAddress:72:9a:65:1d:7:8f ID:1,72:9a:65:1d:7:8f Lease:0x64529727}
I0621 16:36:48.237055   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.41 HWAddress:ae:61:59:6d:f1:c9 ID:1,ae:61:59:6d:f1:c9 Lease:0x64526e21}
I0621 16:36:48.237059   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.40 HWAddress:6e:2f:9c:f4:b6:96 ID:1,6e:2f:9c:f4:b6:96 Lease:0x6450ffe5}
I0621 16:36:48.237063   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.39 HWAddress:72:12:77:32:47:49 ID:1,72:12:77:32:47:49 Lease:0x64523e03}
I0621 16:36:48.237068   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.38 HWAddress:d2:25:e3:2d:31:49 ID:1,d2:25:e3:2d:31:49 Lease:0x6451b25b}
I0621 16:36:48.237072   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.37 HWAddress:5e:a9:62:e6:82:b3 ID:1,5e:a9:62:e6:82:b3 Lease:0x64517e96}
I0621 16:36:48.237083   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.36 HWAddress:9a:5c:d0:fc:fb:3e ID:1,9a:5c:d0:fc:fb:3e Lease:0x64500013}
I0621 16:36:48.237088   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.35 HWAddress:ba:c1:75:8c:94:13 ID:1,ba:c1:75:8c:94:13 Lease:0x644ff9cf}
I0621 16:36:48.237092   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.34 HWAddress:b6:4e:7e:54:f4:e6 ID:1,b6:4e:7e:54:f4:e6 Lease:0x644fb70e}
I0621 16:36:48.237096   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.33 HWAddress:d2:c2:9:16:42:e6 ID:1,d2:c2:9:16:42:e6 Lease:0x644fb655}
I0621 16:36:48.237101   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.32 HWAddress:22:6d:e0:99:d2:87 ID:1,22:6d:e0:99:d2:87 Lease:0x644706a8}
I0621 16:36:48.237106   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.31 HWAddress:ea:5:30:30:d2:15 ID:1,ea:5:30:30:d2:15 Lease:0x6446fd38}
I0621 16:36:48.237110   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.30 HWAddress:82:2e:a7:f9:69:6 ID:1,82:2e:a7:f9:69:6 Lease:0x6446fa8c}
I0621 16:36:48.237115   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.29 HWAddress:4a:ab:16:b1:c9:a9 ID:1,4a:ab:16:b1:c9:a9 Lease:0x6446f6c9}
I0621 16:36:48.237119   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.28 HWAddress:e6:bd:18:1f:e7:4c ID:1,e6:bd:18:1f:e7:4c Lease:0x6446f4d5}
I0621 16:36:48.237124   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.27 HWAddress:ba:22:23:ac:61:85 ID:1,ba:22:23:ac:61:85 Lease:0x6446f41c}
I0621 16:36:48.237129   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.26 HWAddress:2a:8e:ef:28:96:fb ID:1,2a:8e:ef:28:96:fb Lease:0x6446ef65}
I0621 16:36:48.237133   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.25 HWAddress:72:dd:28:42:7b:d3 ID:1,72:dd:28:42:7b:d3 Lease:0x6446eef4}
I0621 16:36:48.237138   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.24 HWAddress:96:89:6b:d4:8:e1 ID:1,96:89:6b:d4:8:e1 Lease:0x6446ee99}
I0621 16:36:48.237142   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.23 HWAddress:4a:e6:7f:50:3f:25 ID:1,4a:e6:7f:50:3f:25 Lease:0x6446ecb5}
I0621 16:36:48.237147   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.22 HWAddress:2e:85:5c:8c:98:7d ID:1,2e:85:5c:8c:98:7d Lease:0x6446ebcf}
I0621 16:36:48.237151   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.21 HWAddress:16:e4:e:cc:b9:83 ID:1,16:e4:e:cc:b9:83 Lease:0x6446e7cb}
I0621 16:36:48.237156   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.20 HWAddress:8a:52:7c:ba:6d:82 ID:1,8a:52:7c:ba:6d:82 Lease:0x644287e4}
I0621 16:36:48.237161   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.19 HWAddress:1e:3c:4e:37:9:ac ID:1,1e:3c:4e:37:9:ac Lease:0x64428247}
I0621 16:36:48.237165   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.18 HWAddress:8e:2c:13:85:8c:a4 ID:1,8e:2c:13:85:8c:a4 Lease:0x64428110}
I0621 16:36:48.237170   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.17 HWAddress:66:53:7:ca:f2:b0 ID:1,66:53:7:ca:f2:b0 Lease:0x64428002}
I0621 16:36:48.237174   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.16 HWAddress:f6:5e:66:47:59:47 ID:1,f6:5e:66:47:59:47 Lease:0x64427baa}
I0621 16:36:48.237179   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.15 HWAddress:4e:63:4b:ba:4f:fb ID:1,4e:63:4b:ba:4f:fb Lease:0x644277cc}
I0621 16:36:48.237188   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.14 HWAddress:92:e8:71:b6:c:e5 ID:1,92:e8:71:b6:c:e5 Lease:0x644270d0}
I0621 16:36:48.237192   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.13 HWAddress:ce:f3:94:6a:24:ce ID:1,ce:f3:94:6a:24:ce Lease:0x644264a6}
I0621 16:36:48.237196   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.12 HWAddress:ca:db:87:22:4:58 ID:1,ca:db:87:22:4:58 Lease:0x644241fe}
I0621 16:36:48.237201   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.11 HWAddress:16:3c:2d:1b:40:d3 ID:1,16:3c:2d:1b:40:d3 Lease:0x64412889}
I0621 16:36:48.237205   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.10 HWAddress:fa:a7:aa:32:8d:43 ID:1,fa:a7:aa:32:8d:43 Lease:0x6441284e}
I0621 16:36:48.237210   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.9 HWAddress:7e:45:19:59:e9:63 ID:1,7e:45:19:59:e9:63 Lease:0x64412806}
I0621 16:36:48.237214   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.8 HWAddress:3a:9d:fb:67:eb:ba ID:1,3a:9d:fb:67:eb:ba Lease:0x64411f96}
I0621 16:36:48.237219   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.7 HWAddress:76:a7:40:98:e9:b7 ID:1,76:a7:40:98:e9:b7 Lease:0x643a7c51}
I0621 16:36:48.237223   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.6 HWAddress:b2:2b:36:17:ec:7a ID:1,b2:2b:36:17:ec:7a Lease:0x642bf388}
I0621 16:36:48.237227   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.5 HWAddress:da:99:b2:56:8d:76 ID:1,da:99:b2:56:8d:76 Lease:0x64297564}
I0621 16:36:48.237232   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.4 HWAddress:16:a2:dc:3e:ee:a3 ID:1,16:a2:dc:3e:ee:a3 Lease:0x6429708c}
I0621 16:36:48.237236   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.3 HWAddress:16:94:64:d9:c7:c8 ID:1,16:94:64:d9:c7:c8 Lease:0x64296d6a}
I0621 16:36:48.237241   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.2 HWAddress:7a:19:8:2a:35:4c ID:1,7a:19:8:2a:35:4c Lease:0x64284dd1}
I0621 16:36:48.237246   19000 main.go:141] libmachine: dhcp entry: {Name:lima-0 IPAddress:192.168.205.2 HWAddress:52:55:55:63:f9:7d ID:1,52:55:55:63:f9:7d Lease:0x64514587}
I0621 16:36:50.237602   19000 main.go:141] libmachine: Attempt 7
I0621 16:36:50.237612   19000 main.go:141] libmachine: Searching for d2:6e:28:d7:5e:56 in /var/db/dhcpd_leases ...
I0621 16:36:50.248017   19000 main.go:141] libmachine: Found 54 entries in /var/db/dhcpd_leases!
I0621 16:36:50.248030   19000 main.go:141] libmachine: dhcp entry: {Name:minikube IPAddress:192.168.105.54 HWAddress:d2:6e:28:d7:5e:56 ID:1,d2:6e:28:d7:5e:56 Lease:0x64944e72}
I0621 16:36:50.248033   19000 main.go:141] libmachine: Found match: d2:6e:28:d7:5e:56
I0621 16:36:50.248065   19000 main.go:141] libmachine: IP: 192.168.105.54
I0621 16:36:50.248069   19000 main.go:141] libmachine: Waiting for VM to start (ssh -p 22 docker@192.168.105.54)...
I0621 16:36:53.307588   19000 machine.go:88] provisioning docker machine ...
I0621 16:36:53.307856   19000 buildroot.go:166] provisioning hostname "cluster-3"
I0621 16:36:53.308385   19000 main.go:141] libmachine: Using SSH client type: native
I0621 16:36:53.309275   19000 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050f5a20] 0x1050f8400 <nil>  [] 0s} 192.168.105.54 22 <nil> <nil>}
I0621 16:36:53.309285   19000 main.go:141] libmachine: About to run SSH command:
sudo hostname cluster-3 && echo "cluster-3" | sudo tee /etc/hostname
I0621 16:36:53.376799   19000 main.go:141] libmachine: SSH cmd err, output: <nil>: cluster-3

I0621 16:36:53.376895   19000 main.go:141] libmachine: Using SSH client type: native
I0621 16:36:53.377204   19000 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050f5a20] 0x1050f8400 <nil>  [] 0s} 192.168.105.54 22 <nil> <nil>}
I0621 16:36:53.377211   19000 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\scluster-3' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 cluster-3/g' /etc/hosts;
			else 
				echo '127.0.1.1 cluster-3' | sudo tee -a /etc/hosts; 
			fi
		fi
I0621 16:36:53.445531   19000 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0621 16:36:53.445542   19000 buildroot.go:172] set auth options {CertDir:/Users/taabaaf1/.minikube CaCertPath:/Users/taabaaf1/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/taabaaf1/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/taabaaf1/.minikube/machines/server.pem ServerKeyPath:/Users/taabaaf1/.minikube/machines/server-key.pem ClientKeyPath:/Users/taabaaf1/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/taabaaf1/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/taabaaf1/.minikube}
I0621 16:36:53.445567   19000 buildroot.go:174] setting up certificates
I0621 16:36:53.445574   19000 provision.go:83] configureAuth start
I0621 16:36:53.445578   19000 provision.go:138] copyHostCerts
I0621 16:36:53.446623   19000 exec_runner.go:144] found /Users/taabaaf1/.minikube/cert.pem, removing ...
I0621 16:36:53.446814   19000 exec_runner.go:207] rm: /Users/taabaaf1/.minikube/cert.pem
I0621 16:36:53.447592   19000 exec_runner.go:151] cp: /Users/taabaaf1/.minikube/certs/cert.pem --> /Users/taabaaf1/.minikube/cert.pem (1127 bytes)
I0621 16:36:53.448119   19000 exec_runner.go:144] found /Users/taabaaf1/.minikube/key.pem, removing ...
I0621 16:36:53.448122   19000 exec_runner.go:207] rm: /Users/taabaaf1/.minikube/key.pem
I0621 16:36:53.448393   19000 exec_runner.go:151] cp: /Users/taabaaf1/.minikube/certs/key.pem --> /Users/taabaaf1/.minikube/key.pem (1675 bytes)
I0621 16:36:53.448904   19000 exec_runner.go:144] found /Users/taabaaf1/.minikube/ca.pem, removing ...
I0621 16:36:53.448910   19000 exec_runner.go:207] rm: /Users/taabaaf1/.minikube/ca.pem
I0621 16:36:53.449095   19000 exec_runner.go:151] cp: /Users/taabaaf1/.minikube/certs/ca.pem --> /Users/taabaaf1/.minikube/ca.pem (1082 bytes)
I0621 16:36:53.449548   19000 provision.go:112] generating server cert: /Users/taabaaf1/.minikube/machines/server.pem ca-key=/Users/taabaaf1/.minikube/certs/ca.pem private-key=/Users/taabaaf1/.minikube/certs/ca-key.pem org=taabaaf1.cluster-3 san=[192.168.105.54 192.168.105.54 localhost 127.0.0.1 minikube cluster-3]
I0621 16:36:53.514631   19000 provision.go:172] copyRemoteCerts
I0621 16:36:53.515134   19000 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0621 16:36:53.515142   19000 sshutil.go:53] new ssh client: &{IP:192.168.105.54 Port:22 SSHKeyPath:/Users/taabaaf1/.minikube/machines/cluster-3/id_rsa Username:docker}
I0621 16:36:53.550222   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0621 16:36:53.577230   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I0621 16:36:53.600652   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0621 16:36:53.611811   19000 provision.go:86] duration metric: configureAuth took 166.228541ms
I0621 16:36:53.611820   19000 buildroot.go:189] setting minikube options for container-runtime
I0621 16:36:53.612391   19000 config.go:182] Loaded profile config "cluster-3": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I0621 16:36:53.612489   19000 main.go:141] libmachine: Using SSH client type: native
I0621 16:36:53.612791   19000 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050f5a20] 0x1050f8400 <nil>  [] 0s} 192.168.105.54 22 <nil> <nil>}
I0621 16:36:53.612794   19000 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0621 16:36:53.680685   19000 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0621 16:36:53.680692   19000 buildroot.go:70] root file system type: tmpfs
I0621 16:36:53.680786   19000 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0621 16:36:53.680875   19000 main.go:141] libmachine: Using SSH client type: native
I0621 16:36:53.681172   19000 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050f5a20] 0x1050f8400 <nil>  [] 0s} 192.168.105.54 22 <nil> <nil>}
I0621 16:36:53.681210   19000 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0621 16:36:53.749191   19000 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0621 16:36:53.749287   19000 main.go:141] libmachine: Using SSH client type: native
I0621 16:36:53.749589   19000 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050f5a20] 0x1050f8400 <nil>  [] 0s} 192.168.105.54 22 <nil> <nil>}
I0621 16:36:53.749596   19000 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0621 16:36:54.212838   19000 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

I0621 16:36:54.212848   19000 machine.go:91] provisioned docker machine in 905.234084ms
I0621 16:36:54.212853   19000 client.go:171] LocalClient.Create took 18.516972709s
I0621 16:36:54.212880   19000 start.go:167] duration metric: libmachine.API.Create for "cluster-3" took 18.5188835s
I0621 16:36:54.212883   19000 start.go:300] post-start starting for "cluster-3" (driver="qemu2")
I0621 16:36:54.212885   19000 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0621 16:36:54.212995   19000 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0621 16:36:54.213004   19000 sshutil.go:53] new ssh client: &{IP:192.168.105.54 Port:22 SSHKeyPath:/Users/taabaaf1/.minikube/machines/cluster-3/id_rsa Username:docker}
I0621 16:36:54.271395   19000 ssh_runner.go:195] Run: cat /etc/os-release
I0621 16:36:54.274656   19000 info.go:137] Remote host: Buildroot 2021.02.12
I0621 16:36:54.274662   19000 filesync.go:126] Scanning /Users/taabaaf1/.minikube/addons for local assets ...
I0621 16:36:54.274856   19000 filesync.go:126] Scanning /Users/taabaaf1/.minikube/files for local assets ...
I0621 16:36:54.274981   19000 start.go:303] post-start completed in 62.093583ms
I0621 16:36:54.278133   19000 profile.go:148] Saving config to /Users/taabaaf1/.minikube/profiles/cluster-3/config.json ...
I0621 16:36:54.279090   19000 start.go:128] duration metric: createHost completed in 18.789454875s
I0621 16:36:54.279163   19000 main.go:141] libmachine: Using SSH client type: native
I0621 16:36:54.279430   19000 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050f5a20] 0x1050f8400 <nil>  [] 0s} 192.168.105.54 22 <nil> <nil>}
I0621 16:36:54.279434   19000 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0621 16:36:54.347681   19000 main.go:141] libmachine: SSH cmd err, output: <nil>: 1687354614.062800377

I0621 16:36:54.347691   19000 fix.go:207] guest clock: 1687354614.062800377
I0621 16:36:54.347695   19000 fix.go:220] Guest: 2023-06-21 16:36:54.062800377 +0300 EEST Remote: 2023-06-21 16:36:54.279098 +0300 EEST m=+19.260273584 (delta=-216.297623ms)
I0621 16:36:54.347707   19000 fix.go:191] guest clock delta is within tolerance: -216.297623ms
I0621 16:36:54.347709   19000 start.go:83] releasing machines lock for "cluster-3", held for 18.85810525s
I0621 16:36:54.348545   19000 ssh_runner.go:195] Run: cat /version.json
I0621 16:36:54.348554   19000 sshutil.go:53] new ssh client: &{IP:192.168.105.54 Port:22 SSHKeyPath:/Users/taabaaf1/.minikube/machines/cluster-3/id_rsa Username:docker}
I0621 16:36:54.348760   19000 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0621 16:36:54.349509   19000 sshutil.go:53] new ssh client: &{IP:192.168.105.54 Port:22 SSHKeyPath:/Users/taabaaf1/.minikube/machines/cluster-3/id_rsa Username:docker}
I0621 16:36:54.388129   19000 ssh_runner.go:195] Run: systemctl --version
I0621 16:36:55.595949   19000 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.247155375s)
W0621 16:36:55.595963   19000 start.go:830] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Failed to connect to registry.k8s.io port 443 after 1204 ms: Connection timed out
I0621 16:36:55.595980   19000 ssh_runner.go:235] Completed: systemctl --version: (1.207821s)
I0621 16:36:55.596330   19000 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0621 16:36:55.596655   19000 out.go:239] ❗  This VM is having trouble accessing https://registry.k8s.io
W0621 16:36:55.596743   19000 out.go:239] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
W0621 16:36:55.607201   19000 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0621 16:36:55.607307   19000 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0621 16:36:55.619212   19000 cni.go:261] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0621 16:36:55.619429   19000 preload.go:132] Checking if preload exists for k8s version v1.26.3 and runtime docker
I0621 16:36:55.619538   19000 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0621 16:36:55.727533   19000 docker.go:639] Got preloaded images: 
I0621 16:36:55.727540   19000 docker.go:645] registry.k8s.io/kube-apiserver:v1.26.3 wasn't preloaded
I0621 16:36:55.727652   19000 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0621 16:36:55.735924   19000 ssh_runner.go:195] Run: which lz4
I0621 16:36:55.751536   19000 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /preloaded.tar.lz4
I0621 16:36:55.754350   19000 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%!s(MISSING) %!y(MISSING)" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0621 16:36:55.754396   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.3-docker-overlay2-arm64.tar.lz4 --> /preloaded.tar.lz4 (346579827 bytes)
I0621 16:36:59.861451   19000 docker.go:603] Took 4.109271 seconds to copy over tarball
I0621 16:36:59.861555   19000 ssh_runner.go:195] Run: sudo tar -I lz4 -C /var -xf /preloaded.tar.lz4
I0621 16:37:01.337973   19000 ssh_runner.go:235] Completed: sudo tar -I lz4 -C /var -xf /preloaded.tar.lz4: (1.47638s)
I0621 16:37:01.337982   19000 ssh_runner.go:146] rm: /preloaded.tar.lz4
I0621 16:37:01.356513   19000 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0621 16:37:01.361528   19000 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2628 bytes)
I0621 16:37:01.368738   19000 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 16:37:01.454513   19000 ssh_runner.go:195] Run: sudo systemctl restart docker
I0621 16:37:03.024890   19000 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.570307417s)
I0621 16:37:03.024987   19000 start.go:481] detecting cgroup driver to use...
I0621 16:37:03.025373   19000 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0621 16:37:03.033530   19000 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0621 16:37:03.037955   19000 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0621 16:37:03.042450   19000 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0621 16:37:03.043167   19000 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0621 16:37:03.047111   19000 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0621 16:37:03.051061   19000 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0621 16:37:03.055267   19000 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0621 16:37:03.060258   19000 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0621 16:37:03.065250   19000 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0621 16:37:03.070244   19000 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0621 16:37:03.074809   19000 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0621 16:37:03.079490   19000 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 16:37:03.165522   19000 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0621 16:37:03.173944   19000 start.go:481] detecting cgroup driver to use...
I0621 16:37:03.174026   19000 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0621 16:37:03.182522   19000 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0621 16:37:03.192982   19000 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0621 16:37:03.207118   19000 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0621 16:37:03.213992   19000 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0621 16:37:03.221254   19000 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0621 16:37:03.263851   19000 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0621 16:37:03.270477   19000 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0621 16:37:03.278931   19000 ssh_runner.go:195] Run: which cri-dockerd
I0621 16:37:03.280843   19000 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0621 16:37:03.285405   19000 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0621 16:37:03.294967   19000 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0621 16:37:03.382039   19000 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0621 16:37:03.457835   19000 docker.go:538] configuring docker to use "cgroupfs" as cgroup driver...
I0621 16:37:03.457852   19000 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0621 16:37:03.468284   19000 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 16:37:03.543948   19000 ssh_runner.go:195] Run: sudo systemctl restart docker
I0621 16:37:04.730799   19000 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.186815416s)
I0621 16:37:04.730946   19000 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0621 16:37:04.826154   19000 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0621 16:37:04.931528   19000 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0621 16:37:05.031266   19000 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 16:37:05.106332   19000 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0621 16:37:05.116677   19000 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 16:37:05.206631   19000 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0621 16:37:05.252499   19000 start.go:528] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0621 16:37:05.252636   19000 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0621 16:37:05.256810   19000 start.go:549] Will wait 60s for crictl version
I0621 16:37:05.256892   19000 ssh_runner.go:195] Run: which crictl
I0621 16:37:05.258866   19000 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0621 16:37:05.289305   19000 start.go:565] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.23
RuntimeApiVersion:  v1alpha2
I0621 16:37:05.289382   19000 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0621 16:37:05.303919   19000 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0621 16:37:05.322473   19000 out.go:204] 🐳  Preparing Kubernetes v1.26.3 on Docker 20.10.23 ...
I0621 16:37:05.323115   19000 ssh_runner.go:195] Run: grep 192.168.105.1	host.minikube.internal$ /etc/hosts
I0621 16:37:05.325301   19000 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.105.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0621 16:37:05.331031   19000 preload.go:132] Checking if preload exists for k8s version v1.26.3 and runtime docker
I0621 16:37:05.331086   19000 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0621 16:37:05.343975   19000 docker.go:639] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.26.3
registry.k8s.io/kube-proxy:v1.26.3
registry.k8s.io/kube-controller-manager:v1.26.3
registry.k8s.io/kube-scheduler:v1.26.3
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0621 16:37:05.343986   19000 docker.go:569] Images already preloaded, skipping extraction
I0621 16:37:05.344058   19000 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0621 16:37:05.358151   19000 docker.go:639] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.26.3
registry.k8s.io/kube-controller-manager:v1.26.3
registry.k8s.io/kube-proxy:v1.26.3
registry.k8s.io/kube-scheduler:v1.26.3
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0621 16:37:05.358171   19000 cache_images.go:84] Images are preloaded, skipping loading
I0621 16:37:05.358484   19000 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0621 16:37:05.374869   19000 cni.go:84] Creating CNI manager for ""
I0621 16:37:05.374877   19000 cni.go:157] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0621 16:37:05.375332   19000 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0621 16:37:05.375344   19000 kubeadm.go:172] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.105.54 APIServerPort:8443 KubernetesVersion:v1.26.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:cluster-3 NodeName:cluster-3 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.105.54"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.105.54 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m]}
I0621 16:37:05.375466   19000 kubeadm.go:177] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.105.54
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "cluster-3"
  kubeletExtraArgs:
    node-ip: 192.168.105.54
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.105.54"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.26.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0621 16:37:05.375698   19000 kubeadm.go:968] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.26.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=cluster-3 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.105.54

[Install]
 config:
{KubernetesVersion:v1.26.3 ClusterName:cluster-3 Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0621 16:37:05.375820   19000 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.26.3
I0621 16:37:05.380300   19000 binaries.go:44] Found k8s binaries, skipping transfer
I0621 16:37:05.380352   19000 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0621 16:37:05.384111   19000 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (372 bytes)
I0621 16:37:05.392968   19000 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0621 16:37:05.400269   19000 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0621 16:37:05.409447   19000 ssh_runner.go:195] Run: grep 192.168.105.54	control-plane.minikube.internal$ /etc/hosts
I0621 16:37:05.411193   19000 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.105.54	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0621 16:37:05.417573   19000 certs.go:56] Setting up /Users/taabaaf1/.minikube/profiles/cluster-3 for IP: 192.168.105.54
I0621 16:37:05.417587   19000 certs.go:186] acquiring lock for shared ca certs: {Name:mk8a650bb0ad359eec29251a6a43f999c893df5f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:05.418009   19000 certs.go:195] skipping minikubeCA CA generation: /Users/taabaaf1/.minikube/ca.key
I0621 16:37:05.418228   19000 certs.go:195] skipping proxyClientCA CA generation: /Users/taabaaf1/.minikube/proxy-client-ca.key
I0621 16:37:05.418265   19000 certs.go:315] generating minikube-user signed cert: /Users/taabaaf1/.minikube/profiles/cluster-3/client.key
I0621 16:37:05.418276   19000 crypto.go:68] Generating cert /Users/taabaaf1/.minikube/profiles/cluster-3/client.crt with IP's: []
I0621 16:37:05.492185   19000 crypto.go:156] Writing cert to /Users/taabaaf1/.minikube/profiles/cluster-3/client.crt ...
I0621 16:37:05.492199   19000 lock.go:35] WriteFile acquiring /Users/taabaaf1/.minikube/profiles/cluster-3/client.crt: {Name:mk76d00ca74969fd8f15214a33cc41e08fd05193 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:05.493177   19000 crypto.go:164] Writing key to /Users/taabaaf1/.minikube/profiles/cluster-3/client.key ...
I0621 16:37:05.493181   19000 lock.go:35] WriteFile acquiring /Users/taabaaf1/.minikube/profiles/cluster-3/client.key: {Name:mka704977c7b95ff0a81f2ac5edd3cf64b9ef7fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:05.493667   19000 certs.go:315] generating minikube signed cert: /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.key.adcc8295
I0621 16:37:05.493677   19000 crypto.go:68] Generating cert /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.crt.adcc8295 with IP's: [192.168.105.54 10.96.0.1 127.0.0.1 10.0.0.1]
I0621 16:37:05.842405   19000 crypto.go:156] Writing cert to /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.crt.adcc8295 ...
I0621 16:37:05.842414   19000 lock.go:35] WriteFile acquiring /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.crt.adcc8295: {Name:mk6a3ea5db0474a4632e2eb32bd29125d3399a4c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:05.843762   19000 crypto.go:164] Writing key to /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.key.adcc8295 ...
I0621 16:37:05.843766   19000 lock.go:35] WriteFile acquiring /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.key.adcc8295: {Name:mk4b6c0549c636a0c82bd0d3f93218f8ce6f5636 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:05.844377   19000 certs.go:333] copying /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.crt.adcc8295 -> /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.crt
I0621 16:37:05.852866   19000 certs.go:337] copying /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.key.adcc8295 -> /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.key
I0621 16:37:05.857424   19000 certs.go:315] generating aggregator signed cert: /Users/taabaaf1/.minikube/profiles/cluster-3/proxy-client.key
I0621 16:37:05.857435   19000 crypto.go:68] Generating cert /Users/taabaaf1/.minikube/profiles/cluster-3/proxy-client.crt with IP's: []
I0621 16:37:05.926443   19000 crypto.go:156] Writing cert to /Users/taabaaf1/.minikube/profiles/cluster-3/proxy-client.crt ...
I0621 16:37:05.926451   19000 lock.go:35] WriteFile acquiring /Users/taabaaf1/.minikube/profiles/cluster-3/proxy-client.crt: {Name:mk7175a0ead7150d20380cc1469f3e55cf6f3c8b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:05.927195   19000 crypto.go:164] Writing key to /Users/taabaaf1/.minikube/profiles/cluster-3/proxy-client.key ...
I0621 16:37:05.927204   19000 lock.go:35] WriteFile acquiring /Users/taabaaf1/.minikube/profiles/cluster-3/proxy-client.key: {Name:mk04fd773d0bf2c93a8c00be8b3e25284924a7e7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:05.932687   19000 certs.go:401] found cert: /Users/taabaaf1/.minikube/certs/Users/taabaaf1/.minikube/certs/ca-key.pem (1675 bytes)
I0621 16:37:05.932855   19000 certs.go:401] found cert: /Users/taabaaf1/.minikube/certs/Users/taabaaf1/.minikube/certs/ca.pem (1082 bytes)
I0621 16:37:05.932900   19000 certs.go:401] found cert: /Users/taabaaf1/.minikube/certs/Users/taabaaf1/.minikube/certs/cert.pem (1127 bytes)
I0621 16:37:05.933019   19000 certs.go:401] found cert: /Users/taabaaf1/.minikube/certs/Users/taabaaf1/.minikube/certs/key.pem (1675 bytes)
I0621 16:37:05.933718   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0621 16:37:05.947184   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/profiles/cluster-3/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0621 16:37:05.958353   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/profiles/cluster-3/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0621 16:37:05.972103   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/profiles/cluster-3/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0621 16:37:05.985512   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0621 16:37:05.998380   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0621 16:37:06.017293   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0621 16:37:06.031123   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0621 16:37:06.043779   19000 ssh_runner.go:362] scp /Users/taabaaf1/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0621 16:37:06.055225   19000 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0621 16:37:06.064611   19000 ssh_runner.go:195] Run: openssl version
I0621 16:37:06.068229   19000 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0621 16:37:06.072903   19000 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0621 16:37:06.075404   19000 certs.go:444] hashing: -rw-r--r-- 1 root root 1111 Mar 14 08:26 /usr/share/ca-certificates/minikubeCA.pem
I0621 16:37:06.075456   19000 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0621 16:37:06.077958   19000 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0621 16:37:06.082467   19000 kubeadm.go:401] StartCluster: {Name:cluster-3 KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.30.1-arm64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39@sha256:bf2d9f1e9d837d8deea073611d2605405b6be904647d97ebd9b12045ddfe1106 Memory:8192 CPUs:4 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:cluster-3 Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.105.54 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/homebrew/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/opt/homebrew/var/run/socket_vmnet StaticIP:}
I0621 16:37:06.082565   19000 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0621 16:37:06.093379   19000 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0621 16:37:06.097813   19000 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0621 16:37:06.102456   19000 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0621 16:37:06.106348   19000 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0621 16:37:06.106617   19000 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0621 16:37:06.147689   19000 kubeadm.go:322] [init] Using Kubernetes version: v1.26.3
I0621 16:37:06.147714   19000 kubeadm.go:322] [preflight] Running pre-flight checks
I0621 16:37:06.255095   19000 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0621 16:37:06.255169   19000 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0621 16:37:06.255253   19000 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0621 16:37:06.333833   19000 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0621 16:37:06.339114   19000 out.go:204]     ▪ Generating certificates and keys ...
I0621 16:37:06.339163   19000 kubeadm.go:322] [certs] Using existing ca certificate authority
I0621 16:37:06.339219   19000 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0621 16:37:06.461348   19000 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I0621 16:37:06.763932   19000 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I0621 16:37:06.951383   19000 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I0621 16:37:07.104546   19000 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I0621 16:37:07.267429   19000 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I0621 16:37:07.267517   19000 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [cluster-3 localhost] and IPs [192.168.105.54 127.0.0.1 ::1]
I0621 16:37:07.489286   19000 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I0621 16:37:07.489576   19000 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [cluster-3 localhost] and IPs [192.168.105.54 127.0.0.1 ::1]
I0621 16:37:07.593227   19000 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I0621 16:37:07.649389   19000 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I0621 16:37:07.747728   19000 kubeadm.go:322] [certs] Generating "sa" key and public key
I0621 16:37:07.747812   19000 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0621 16:37:07.844363   19000 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I0621 16:37:08.006102   19000 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0621 16:37:08.089696   19000 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0621 16:37:08.168392   19000 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0621 16:37:08.178810   19000 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0621 16:37:08.178878   19000 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0621 16:37:08.178913   19000 kubeadm.go:322] [kubelet-start] Starting the kubelet
I0621 16:37:08.268492   19000 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0621 16:37:08.272777   19000 out.go:204]     ▪ Booting up control plane ...
I0621 16:37:08.272859   19000 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0621 16:37:08.272965   19000 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0621 16:37:08.273009   19000 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0621 16:37:08.273060   19000 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0621 16:37:08.273140   19000 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0621 16:37:12.771578   19000 kubeadm.go:322] [apiclient] All control plane components are healthy after 4.501233 seconds
I0621 16:37:12.771643   19000 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0621 16:37:12.781953   19000 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0621 16:37:13.299875   19000 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I0621 16:37:13.300131   19000 kubeadm.go:322] [mark-control-plane] Marking the node cluster-3 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0621 16:37:13.808175   19000 kubeadm.go:322] [bootstrap-token] Using token: bn13dg.81ojd54nra0zvcr1
I0621 16:37:13.813980   19000 out.go:204]     ▪ Configuring RBAC rules ...
I0621 16:37:13.814091   19000 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0621 16:37:13.814174   19000 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0621 16:37:13.818209   19000 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0621 16:37:13.820141   19000 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0621 16:37:13.822045   19000 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0621 16:37:13.824845   19000 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0621 16:37:13.831984   19000 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0621 16:37:14.036365   19000 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I0621 16:37:14.217544   19000 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I0621 16:37:14.218154   19000 kubeadm.go:322] 
I0621 16:37:14.218200   19000 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I0621 16:37:14.218231   19000 kubeadm.go:322] 
I0621 16:37:14.218290   19000 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I0621 16:37:14.218293   19000 kubeadm.go:322] 
I0621 16:37:14.218309   19000 kubeadm.go:322]   mkdir -p $HOME/.kube
I0621 16:37:14.218344   19000 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0621 16:37:14.218369   19000 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0621 16:37:14.218371   19000 kubeadm.go:322] 
I0621 16:37:14.218432   19000 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I0621 16:37:14.218435   19000 kubeadm.go:322] 
I0621 16:37:14.218485   19000 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0621 16:37:14.218496   19000 kubeadm.go:322] 
I0621 16:37:14.218566   19000 kubeadm.go:322] You should now deploy a pod network to the cluster.
I0621 16:37:14.218605   19000 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0621 16:37:14.218641   19000 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0621 16:37:14.218642   19000 kubeadm.go:322] 
I0621 16:37:14.218706   19000 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I0621 16:37:14.218753   19000 kubeadm.go:322] and service account keys on each node and then running the following as root:
I0621 16:37:14.218756   19000 kubeadm.go:322] 
I0621 16:37:14.218817   19000 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token bn13dg.81ojd54nra0zvcr1 \
I0621 16:37:14.218884   19000 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:647ad4fffbe410f049c226b74c6a028e28cf087d005f92752436b2f72d738352 \
I0621 16:37:14.218915   19000 kubeadm.go:322] 	--control-plane 
I0621 16:37:14.218919   19000 kubeadm.go:322] 
I0621 16:37:14.218977   19000 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I0621 16:37:14.218978   19000 kubeadm.go:322] 
I0621 16:37:14.219029   19000 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token bn13dg.81ojd54nra0zvcr1 \
I0621 16:37:14.219090   19000 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:647ad4fffbe410f049c226b74c6a028e28cf087d005f92752436b2f72d738352 
I0621 16:37:14.220914   19000 kubeadm.go:322] W0621 13:37:05.859599    1416 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
I0621 16:37:14.220971   19000 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0621 16:37:14.220985   19000 cni.go:84] Creating CNI manager for ""
I0621 16:37:14.221001   19000 cni.go:157] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0621 16:37:14.225193   19000 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0621 16:37:14.237429   19000 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0621 16:37:14.246419   19000 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0621 16:37:14.254957   19000 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0621 16:37:14.255082   19000 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.26.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0621 16:37:14.255667   19000 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.26.3/kubectl label nodes minikube.k8s.io/version=v1.30.1 minikube.k8s.io/commit=08896fd1dc362c097c925146c4a0d0dac715ace0 minikube.k8s.io/name=cluster-3 minikube.k8s.io/updated_at=2023_06_21T16_37_14_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0621 16:37:14.311722   19000 ops.go:34] apiserver oom_adj: -16
I0621 16:37:14.311734   19000 kubeadm.go:1073] duration metric: took 56.764791ms to wait for elevateKubeSystemPrivileges.
I0621 16:37:14.326152   19000 kubeadm.go:403] StartCluster complete in 8.24353775s
I0621 16:37:14.326172   19000 settings.go:142] acquiring lock: {Name:mk4910690997a1cb7f07d1a39f97123ba6005bf0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:14.326303   19000 settings.go:150] Updating kubeconfig:  /Users/taabaaf1/.kube/config
I0621 16:37:14.327898   19000 lock.go:35] WriteFile acquiring /Users/taabaaf1/.kube/config: {Name:mke969ed6bc7ac7a05f9d9ad9d916851f7d2a200 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 16:37:14.329031   19000 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0621 16:37:14.329189   19000 addons.go:496] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0621 16:37:14.329224   19000 addons.go:66] Setting storage-provisioner=true in profile "cluster-3"
I0621 16:37:14.329227   19000 addons.go:66] Setting default-storageclass=true in profile "cluster-3"
I0621 16:37:14.329232   19000 addons.go:228] Setting addon storage-provisioner=true in "cluster-3"
I0621 16:37:14.329235   19000 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "cluster-3"
I0621 16:37:14.329739   19000 host.go:66] Checking if "cluster-3" exists ...
I0621 16:37:14.345452   19000 config.go:182] Loaded profile config "cluster-3": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I0621 16:37:14.351462   19000 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0621 16:37:14.355413   19000 addons.go:420] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0621 16:37:14.355418   19000 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0621 16:37:14.355429   19000 sshutil.go:53] new ssh client: &{IP:192.168.105.54 Port:22 SSHKeyPath:/Users/taabaaf1/.minikube/machines/cluster-3/id_rsa Username:docker}
I0621 16:37:14.368193   19000 addons.go:228] Setting addon default-storageclass=true in "cluster-3"
I0621 16:37:14.368215   19000 host.go:66] Checking if "cluster-3" exists ...
I0621 16:37:14.369356   19000 addons.go:420] installing /etc/kubernetes/addons/storageclass.yaml
I0621 16:37:14.369362   19000 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0621 16:37:14.369370   19000 sshutil.go:53] new ssh client: &{IP:192.168.105.54 Port:22 SSHKeyPath:/Users/taabaaf1/.minikube/machines/cluster-3/id_rsa Username:docker}
I0621 16:37:14.381825   19000 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.105.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.26.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0621 16:37:14.420857   19000 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0621 16:37:14.435908   19000 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0621 16:37:14.827374   19000 start.go:916] {"host.minikube.internal": 192.168.105.1} host record injected into CoreDNS's ConfigMap
I0621 16:37:14.867882   19000 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I0621 16:37:14.875865   19000 addons.go:499] enable addons completed in 546.662833ms: enabled=[default-storageclass storage-provisioner]
I0621 16:37:14.884306   19000 kapi.go:248] "coredns" deployment in "kube-system" namespace and "cluster-3" context rescaled to 1 replicas
I0621 16:37:14.884325   19000 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.105.54 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0621 16:37:14.887904   19000 out.go:177] 🔎  Verifying Kubernetes components...
I0621 16:37:14.895070   19000 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0621 16:37:14.915538   19000 api_server.go:51] waiting for apiserver process to appear ...
I0621 16:37:14.915606   19000 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0621 16:37:14.921790   19000 api_server.go:71] duration metric: took 37.451792ms to wait for apiserver process to appear ...
I0621 16:37:14.921796   19000 api_server.go:87] waiting for apiserver healthz status ...
I0621 16:37:14.922080   19000 api_server.go:252] Checking apiserver healthz at https://192.168.105.54:8443/healthz ...
I0621 16:37:14.927709   19000 api_server.go:278] https://192.168.105.54:8443/healthz returned 200:
ok
I0621 16:37:14.928827   19000 api_server.go:140] control plane version: v1.26.3
I0621 16:37:14.928832   19000 api_server.go:130] duration metric: took 7.033375ms to wait for apiserver health ...
I0621 16:37:14.928838   19000 system_pods.go:43] waiting for kube-system pods to appear ...
I0621 16:37:14.933619   19000 system_pods.go:59] 5 kube-system pods found
I0621 16:37:14.933626   19000 system_pods.go:61] "etcd-cluster-3" [417ea5fc-785e-418f-a58f-4eba0914a751] Pending
I0621 16:37:14.933628   19000 system_pods.go:61] "kube-apiserver-cluster-3" [352a9c61-fdb5-400e-80bf-5f35017f3ad5] Pending
I0621 16:37:14.933630   19000 system_pods.go:61] "kube-controller-manager-cluster-3" [02e87000-1b26-4937-a33e-e8b591101b5e] Pending
I0621 16:37:14.933632   19000 system_pods.go:61] "kube-scheduler-cluster-3" [7ba326fc-2a80-4831-af93-0dc47ab7cba1] Pending
I0621 16:37:14.933634   19000 system_pods.go:61] "storage-provisioner" [4a967e59-7825-49ca-b662-9ff29a9119be] Pending
I0621 16:37:14.933636   19000 system_pods.go:74] duration metric: took 4.794375ms to wait for pod list to return data ...
I0621 16:37:14.933639   19000 kubeadm.go:578] duration metric: took 49.302333ms to wait for : map[apiserver:true system_pods:true] ...
I0621 16:37:14.933646   19000 node_conditions.go:102] verifying NodePressure condition ...
I0621 16:37:14.936561   19000 node_conditions.go:122] node storage ephemeral capacity is 17784760Ki
I0621 16:37:14.936690   19000 node_conditions.go:123] node cpu capacity is 4
I0621 16:37:14.936696   19000 node_conditions.go:105] duration metric: took 3.048208ms to run NodePressure ...
I0621 16:37:14.936701   19000 start.go:228] waiting for startup goroutines ...
I0621 16:37:14.936704   19000 start.go:233] waiting for cluster config update ...
I0621 16:37:14.936709   19000 start.go:242] writing updated cluster config ...
I0621 16:37:14.937219   19000 ssh_runner.go:195] Run: rm -f paused
I0621 16:37:15.335402   19000 start.go:568] kubectl: 1.26.3, cluster: 1.26.3 (minor skew: 0)
I0621 16:37:15.339976   19000 out.go:177] 🏄  Done! kubectl is now configured to use "cluster-3" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Journal begins at Sun 2023-06-18 12:50:24 UTC, ends at Wed 2023-06-21 13:40:10 UTC. --
Jun 21 13:31:25 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:25.699283765Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:31:25 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:25.993981612Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jun 21 13:31:25 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:25.994076487Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:31:25 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:25.994094487Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jun 21 13:31:25 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:25.994814531Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:31:26 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:26.296955523Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jun 21 13:31:26 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:26.297281607Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:31:26 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:26.297301523Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jun 21 13:31:26 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:26.297312357Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:31:26 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:26.673785791Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jun 21 13:31:26 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:26.674036208Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:31:26 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:26.674136250Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jun 21 13:31:26 cluster-2 dockerd[2581520]: time="2023-06-21T13:31:26.674180917Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:31:39 cluster-2 dockerd[2581510]: time="2023-06-21T13:31:39.455549425Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:31:39 cluster-2 dockerd[2581510]: time="2023-06-21T13:31:39.455606300Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:31:39 cluster-2 dockerd[2581510]: time="2023-06-21T13:31:39.457871806Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:31:54 cluster-2 dockerd[2581510]: time="2023-06-21T13:31:54.473674897Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:31:54 cluster-2 dockerd[2581510]: time="2023-06-21T13:31:54.473734980Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:31:54 cluster-2 dockerd[2581510]: time="2023-06-21T13:31:54.476984197Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:32:09 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:09.486284228Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:32:09 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:09.486361562Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:32:09 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:09.489512236Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:32:13 cluster-2 cri-dockerd[2584207]: time="2023-06-21T13:32:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-patch-bcnqc_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"45b81dc8b341aba69f7a9635dc4f994f3a6e451baab8451440c172e511572dd4\""
Jun 21 13:32:13 cluster-2 cri-dockerd[2584207]: time="2023-06-21T13:32:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-admission-create-cb587_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1341575d5a1623c9738326d3fd5537bba39065ebc61d899b08235c7afd1b2bee\""
Jun 21 13:32:24 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:24.513002220Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:32:24 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:24.513399096Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:32:24 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:24.516996397Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:32:50 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:50.686837810Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:32:50 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:50.687113769Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:32:50 cluster-2 dockerd[2581510]: time="2023-06-21T13:32:50.692535283Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:33:07 cluster-2 dockerd[2581510]: time="2023-06-21T13:33:07.688739876Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:33:07 cluster-2 dockerd[2581510]: time="2023-06-21T13:33:07.688927627Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:33:07 cluster-2 dockerd[2581510]: time="2023-06-21T13:33:07.691215924Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:33:42 cluster-2 dockerd[2581510]: time="2023-06-21T13:33:42.689984130Z" level=info msg="Error logging in to endpoint, trying next endpoint" error="Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:33:42 cluster-2 dockerd[2581510]: time="2023-06-21T13:33:42.690061214Z" level=error msg="Handler for POST /v1.41/auth returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:33:49 cluster-2 dockerd[2581510]: time="2023-06-21T13:33:49.726244740Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp 34.205.13.154:443: i/o timeout"
Jun 21 13:33:49 cluster-2 dockerd[2581510]: time="2023-06-21T13:33:49.726346949Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp 34.205.13.154:443: i/o timeout"
Jun 21 13:33:49 cluster-2 dockerd[2581510]: time="2023-06-21T13:33:49.728441121Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp 34.205.13.154:443: i/o timeout"
Jun 21 13:34:09 cluster-2 dockerd[2581510]: time="2023-06-21T13:34:09.000119189Z" level=info msg="Error logging in to endpoint, trying next endpoint" error="Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:34:09 cluster-2 dockerd[2581510]: time="2023-06-21T13:34:09.000203773Z" level=error msg="Handler for POST /v1.41/auth returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:35:18 cluster-2 dockerd[2581520]: time="2023-06-21T13:35:18.962736986Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jun 21 13:35:18 cluster-2 dockerd[2581520]: time="2023-06-21T13:35:18.962801695Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:35:18 cluster-2 dockerd[2581520]: time="2023-06-21T13:35:18.962809445Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jun 21 13:35:18 cluster-2 dockerd[2581520]: time="2023-06-21T13:35:18.962814445Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jun 21 13:35:19 cluster-2 dockerd[2581510]: time="2023-06-21T13:35:19.241486792Z" level=info msg="ignoring event" container=96dbe0c9d6c8e60fc34abf2a61b76d0518f1272bf0fe2ab33edfc332ac6c7a2d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 21 13:35:19 cluster-2 dockerd[2581520]: time="2023-06-21T13:35:19.241901209Z" level=info msg="shim disconnected" id=96dbe0c9d6c8e60fc34abf2a61b76d0518f1272bf0fe2ab33edfc332ac6c7a2d namespace=moby
Jun 21 13:35:19 cluster-2 dockerd[2581520]: time="2023-06-21T13:35:19.242299919Z" level=warning msg="cleaning up after shim disconnected" id=96dbe0c9d6c8e60fc34abf2a61b76d0518f1272bf0fe2ab33edfc332ac6c7a2d namespace=moby
Jun 21 13:35:19 cluster-2 dockerd[2581520]: time="2023-06-21T13:35:19.242396919Z" level=info msg="cleaning up dead shim" namespace=moby
Jun 21 13:35:36 cluster-2 dockerd[2581510]: time="2023-06-21T13:35:36.703530580Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:35:36 cluster-2 dockerd[2581510]: time="2023-06-21T13:35:36.703569289Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:35:36 cluster-2 dockerd[2581510]: time="2023-06-21T13:35:36.706777964Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Jun 21 13:35:51 cluster-2 dockerd[2581510]: time="2023-06-21T13:35:51.717935817Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp 44.205.64.79:443: i/o timeout"
Jun 21 13:35:51 cluster-2 dockerd[2581510]: time="2023-06-21T13:35:51.717971692Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp 44.205.64.79:443: i/o timeout"
Jun 21 13:35:51 cluster-2 dockerd[2581510]: time="2023-06-21T13:35:51.719489738Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp 44.205.64.79:443: i/o timeout"
Jun 21 13:37:38 cluster-2 dockerd[2581510]: time="2023-06-21T13:37:38.902044768Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:37:38 cluster-2 dockerd[2581510]: time="2023-06-21T13:37:38.902150935Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:37:38 cluster-2 dockerd[2581510]: time="2023-06-21T13:37:38.905435236Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded"
Jun 21 13:38:42 cluster-2 dockerd[2581510]: time="2023-06-21T13:38:42.694807823Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp 44.205.64.79:443: i/o timeout"
Jun 21 13:38:42 cluster-2 dockerd[2581510]: time="2023-06-21T13:38:42.694915406Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp 44.205.64.79:443: i/o timeout"
Jun 21 13:38:42 cluster-2 dockerd[2581510]: time="2023-06-21T13:38:42.698147040Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp 44.205.64.79:443: i/o timeout"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                              CREATED             STATE               NAME                                 ATTEMPT             POD ID
c30d14cb18b4a       53b0975a67956                                                                                                      8 minutes ago       Running             appfrontend-app                      1                   a47676c7e15a5
4151665648e01       3c326264b5996                                                                                                      8 minutes ago       Running             postgres                             2                   8797bc305d1ea
a52aa07faea94       c0cf76f57a9de                                                                                                      8 minutes ago       Running             jhipster-registry                    1                   d73f0d295ea2c
d68dd66b9972a       fb7afe0c66e3f                                                                                                      8 minutes ago       Running             jaeger-operator                      2                   61fa46993d128
d5b1c62d63dce       c0cf76f57a9de                                                                                                      8 minutes ago       Running             jhipster-registry                    1                   4df349e257fe9
5469bdd627b29       9c783c3f8f50f                                                                                                      8 minutes ago       Running             cert-manager-webhook                 11                  41d174a340705
06142eed51190       b19406328e70d                                                                                                      8 minutes ago       Running             coredns                              11                  c135f7fc43ae1
5ebe08dcf0470       c859f97be11ac                                                                                                      8 minutes ago       Running             kube-proxy                           9                   eee1b88960978
45839f85d87da       fb7afe0c66e3f                                                                                                      8 minutes ago       Running             jaeger-operator                      3                   9d4d999212e75
5c3e33f0ca94c       ba04bb24b9575                                                                                                      8 minutes ago       Running             storage-provisioner                  8                   0d8b1f16eca17
ac9c96c965d1c       f23e287a379b3                                                                                                      8 minutes ago       Running             cert-manager-cainjector              10                  2f52280e9f365
e1c33210ae9b5       9aa0553f3e185                                                                                                      8 minutes ago       Running             kube-rbac-proxy                      4                   eefce7ab2cab8
b6735063e8501       e3cde79d245d4                                                                                                      8 minutes ago       Running             manager                              4                   eefce7ab2cab8
44b2c8cfc66c9       fa167119f9a55                                                                                                      8 minutes ago       Running             kube-scheduler                       9                   a71cccc640987
b7956d7775881       ef24580282403                                                                                                      8 minutes ago       Running             etcd                                 10                  34f4bac47ecfd
991d8f2aacde9       3b6ac91ff8d39                                                                                                      8 minutes ago       Running             kube-controller-manager              9                   60869df1d57f3
a2fc7c591bcd1       3f1ae10c5c85d                                                                                                      9 minutes ago       Running             kube-apiserver                       9                   2e83d38d1f785
2cd2105d3c884       b19406328e70d                                                                                                      9 minutes ago       Exited              coredns                              10                  67a0c62ed9b73
a56088cc00755       1b591e4d0738f                                                                                                      9 minutes ago       Running             otc-container                        1                   3b0248f5dec3d
f2492dea2b8cf       b7b9e5f2f3ff8                                                                                                      9 minutes ago       Running             controller                           1                   b895f841f993b
172b00d859776       fbc85505a4890                                                                                                      9 minutes ago       Running             cert-manager-controller              9                   858054781d85b
b1f71126a2960       f23e287a379b3                                                                                                      9 minutes ago       Exited              cert-manager-cainjector              9                   2f52280e9f365
7075d2a632c92       5cbc06cc5ff0b                                                                                                      9 minutes ago       Exited              opentelemetry-auto-instrumentation   0                   d73f0d295ea2c
ef35c95003ca7       5cbc06cc5ff0b                                                                                                      9 minutes ago       Exited              opentelemetry-auto-instrumentation   0                   a47676c7e15a5
68646c0687551       9c783c3f8f50f                                                                                                      9 minutes ago       Exited              cert-manager-webhook                 10                  41d174a340705
d3fb473f4fff2       5cbc06cc5ff0b                                                                                                      9 minutes ago       Exited              opentelemetry-auto-instrumentation   0                   4df349e257fe9
e12bf8613a606       9aa0553f3e185                                                                                                      9 minutes ago       Running             kube-rbac-proxy                      2                   9d4d999212e75
de66ceb6fbc2a       5cbc06cc5ff0b                                                                                                      9 minutes ago       Exited              opentelemetry-auto-instrumentation   0                   3b0248f5dec3d
16b1e28aa245e       5cbc06cc5ff0b                                                                                                      9 minutes ago       Exited              opentelemetry-auto-instrumentation   0                   5797828913b15
2ac6f36c25861       5cbc06cc5ff0b                                                                                                      9 minutes ago       Exited              opentelemetry-auto-instrumentation   2                   61fa46993d128
a5650cf153a09       fb7afe0c66e3f                                                                                                      9 minutes ago       Exited              jaeger-operator                      2                   9d4d999212e75
46640f019c953       5cbc06cc5ff0b                                                                                                      9 minutes ago       Exited              opentelemetry-auto-instrumentation   2                   8797bc305d1ea
77f8c98fcbb5d       1b591e4d0738f                                                                                                      9 minutes ago       Running             otc-container                        1                   07e3f09d04402
ff0a1e4e0eb50       1b591e4d0738f                                                                                                      9 minutes ago       Running             otc-container                        1                   5ee278892bc36
beb8c4e1cf0a7       3b6ac91ff8d39                                                                                                      9 minutes ago       Exited              kube-controller-manager              8                   b1ef00b9c946e
fff4b256c8fca       fa167119f9a55                                                                                                      9 minutes ago       Exited              kube-scheduler                       8                   9bcc58b5cff65
96d767e042eb9       ba04bb24b9575                                                                                                      9 minutes ago       Exited              storage-provisioner                  7                   c1c8ecd5b67b7
94b8514a3ab56       ef24580282403                                                                                                      9 minutes ago       Exited              etcd                                 9                   cf1635540f013
478fef9872d07       c859f97be11ac                                                                                                      9 minutes ago       Exited              kube-proxy                           8                   ab4d01b857389
fba3763716787       9aa0553f3e185                                                                                                      9 minutes ago       Created             kube-rbac-proxy                      3                   4538ce3565b5e
fa653f1b23c82       e3cde79d245d4                                                                                                      9 minutes ago       Created             manager                              3                   4538ce3565b5e
0883ceecaa0ab       3c326264b5996                                                                                                      9 minutes ago       Created             postgres                             1                   a40e1870a64cc
139e804de382b       fb7afe0c66e3f                                                                                                      9 minutes ago       Created             jaeger-operator                      1                   a6e3411003b4c
47add03225075       9aa0553f3e185                                                                                                      9 minutes ago       Exited              kube-rbac-proxy                      1                   1961da46e9bfd
f8365cda492cf       3f1ae10c5c85d                                                                                                      9 minutes ago       Exited              kube-apiserver                       8                   4120c0117fc85
581a8f291e62a       jaegertracing/all-in-one@sha256:89f04319ce418edf8b7bc0ec86a77b5ebb9b76cf4a0b0e10fa4bf54d67e377ad                   2 days ago          Exited              jaeger                               0                   8264e25459562
6194a5265e763       53b0975a67956                                                                                                      2 days ago          Exited              appfrontend-app                      0                   06e700602ba33
26d6cf82a18bd       058f47f74801f                                                                                                      2 days ago          Exited              appbackend-app                       0                   8d6dbcfad1ff8
50daf891548e8       registry.k8s.io/ingress-nginx/controller@sha256:7612338342a1e7b8090bef78f2a04fffcadd548ccaabe8a47bf7758ff549a5f7   2 days ago          Exited              controller                           0                   399d5308651a2
029345217d8f1       c0cf76f57a9de                                                                                                      3 days ago          Exited              jhipster-registry                    0                   6a4b79852c94f
964b804f3d011       c0cf76f57a9de                                                                                                      3 days ago          Exited              jhipster-registry                    0                   9baef7109906a
18e0cdcb9fbbb       1b591e4d0738f                                                                                                      3 days ago          Exited              otc-container                        0                   37b26a17fa2bf
a11eaaaeea0cf       1b591e4d0738f                                                                                                      3 days ago          Exited              otc-container                        0                   f96c547d82367
f14d1cb967c94       1b591e4d0738f                                                                                                      3 days ago          Exited              otc-container                        0                   e1e2313fb9b34
be7e9a60a84a3       fbc85505a4890                                                                                                      3 days ago          Exited              cert-manager-controller              8                   d434df5e0f0a3

* 
* ==> controller_ingress [50daf891548e] <==
* 192.168.105.1 - - [18/Jun/2023:14:22:17 +0000] "GET /login HTTP/1.1" 200 5644 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 452 0.010 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 5644 0.010 200 349c5cf8cdf6a231125800a263242395
192.168.105.1 - - [18/Jun/2023:14:25:41 +0000] "GET / HTTP/1.1" 200 5644 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 447 0.002 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 5644 0.002 200 ecae89e28e9a80b83a2fed628e908291
192.168.105.1 - - [18/Jun/2023:14:25:41 +0000] "GET /content/css/loading.css HTTP/1.1" 200 3477 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 356 0.002 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 3477 0.002 200 833d0b068835b9ddc8125ac9576fe89d
192.168.105.1 - - [18/Jun/2023:14:25:41 +0000] "GET /runtime.js HTTP/1.1" 200 52130 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 328 0.012 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 52130 0.012 200 0e30e878d4a9d39d1500532c9b12fd6b
192.168.105.1 - - [18/Jun/2023:14:25:41 +0000] "GET /main.js HTTP/1.1" 200 1431 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 356 0.014 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 1431 0.014 200 c26e209c94d024f96ab11988cd662d6b
192.168.105.1 - - [18/Jun/2023:14:25:41 +0000] "GET /styles.js HTTP/1.1" 200 225928 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 327 0.016 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 225928 0.016 200 be81fac0d5ad0a895154818ff9158bdd
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /styles.css HTTP/1.1" 200 245685 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 343 0.034 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 245685 0.034 200 8a69cf3f30d1691a4359774e5d697ce9
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /polyfills.js HTTP/1.1" 200 1203077 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 361 0.083 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 1203077 0.083 200 1d8cce249552a351669d4296919a6d55
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /vendor.js HTTP/1.1" 200 1878086 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 358 0.101 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 1878086 0.101 200 d219901a96f383da1b84b95cf5bb5b1e
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /content/images/logo-jhipster.png HTTP/1.1" 200 605 "http://192.168.105.52/content/css/loading.css" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 434 0.002 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 605 0.002 200 72e5613c4496c58c7ede4ea15f8ed933
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /src_main_webapp_bootstrap_ts.js HTTP/1.1" 200 3560447 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 380 0.062 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 3560447 0.062 200 43e426f2559f894cc359511e6af32f04
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /management/info HTTP/1.1" 504 79 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 363 0.006 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 79 0.006 504 2c2d1052db5de5d006c13d2d5d18c826
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /logo-jhipster.png HTTP/1.1" 200 605 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 396 0.001 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 605 0.001 200 6f0fc38b640c7907e14d7e73916b1238
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /api/account HTTP/1.1" 401 228 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 359 0.057 [audit-log-appbackend-8080] [] 10.244.0.128:8080 228 0.057 401 72c84d979a19ba8e5c86223db21322de
192.168.105.1 - - [18/Jun/2023:14:25:42 +0000] "GET /jhipster_family_member_1.svg HTTP/1.1" 200 227303 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 407 0.014 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 227303 0.014 200 dd87c110a96e0582128593e8d6dd9b66
192.168.105.1 - - [18/Jun/2023:14:25:43 +0000] "GET /src_main_webapp_app_login_login_module_ts.js HTTP/1.1" 200 16650 "http://192.168.105.52/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 393 0.002 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 16650 0.002 200 472edc689f15ed7a636bdd0b2b43d980
192.168.105.1 - - [18/Jun/2023:14:25:43 +0000] "GET /api/account HTTP/1.1" 401 228 "http://192.168.105.52/login" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 364 0.006 [audit-log-appbackend-8080] [] 10.244.0.128:8080 228 0.006 401 43863edbde13582d0e2ac05f4ff16dd1
192.168.105.1 - - [18/Jun/2023:14:25:48 +0000] "POST /api/authenticate HTTP/1.1" 200 222 "http://192.168.105.52/login" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 511 0.730 [audit-log-appbackend-8080] [] 10.244.0.128:8080 223 0.730 200 3731846149b5b2c0646599d1caa945eb
192.168.105.1 - - [18/Jun/2023:14:25:48 +0000] "GET /api/account HTTP/1.1" 200 208 "http://192.168.105.52/login" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 579 0.019 [audit-log-appbackend-8080] [] 10.244.0.128:8080 209 0.019 200 f47d4057ec19e9e25f672ada501d5ba6
192.168.105.1 - - [18/Jun/2023:14:25:52 +0000] "GET /ng-cli-ws HTTP/1.1" 101 209 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" 514 10.028 [audit-log-appfrontend-9000] [] 10.244.0.130:9000 0 10.028 101 bea062cadca385b113db7b69c77af762
I0618 14:19:46.410550       7 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-6cc5ccb977-2slw4", UID:"f7528079-418f-4e20-90d9-6d73424a7a8a", APIVersion:"v1", ResourceVersion:"61291", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0618 14:20:05.546717       7 controller.go:1044] Error obtaining Endpoints for Service "audit-log/appbackend": no object matching key "audit-log/appbackend" in local store
I0618 14:20:05.567780       7 admission.go:149] processed ingress via admission controller {testedIngressLength:1 testedIngressTime:0.021s renderingIngressLength:1 renderingIngressTime:0s admissionTime:21.5kBs testedConfigurationSize:0.021}
I0618 14:20:05.567875       7 main.go:100] "successfully validated configuration, accepting" ingress="audit-log/backend-ingress"
I0618 14:20:05.573052       7 store.go:433] "Found valid IngressClass" ingress="audit-log/backend-ingress" ingressclass="nginx"
W0618 14:20:05.573308       7 controller.go:1044] Error obtaining Endpoints for Service "audit-log/appbackend": no object matching key "audit-log/appbackend" in local store
I0618 14:20:05.573374       7 controller.go:189] "Configuration changes detected, backend reload required"
I0618 14:20:05.576131       7 event.go:285] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"audit-log", Name:"backend-ingress", UID:"e5f28ca7-986e-4493-afe9-669ca07e45c2", APIVersion:"networking.k8s.io/v1", ResourceVersion:"62218", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0618 14:20:05.612357       7 controller.go:206] "Backend successfully reloaded"
I0618 14:20:05.612757       7 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-6cc5ccb977-2slw4", UID:"f7528079-418f-4e20-90d9-6d73424a7a8a", APIVersion:"v1", ResourceVersion:"61291", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0618 14:20:08.907757       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
I0618 14:20:08.907826       7 controller.go:189] "Configuration changes detected, backend reload required"
I0618 14:20:08.936814       7 controller.go:206] "Backend successfully reloaded"
I0618 14:20:08.937086       7 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-6cc5ccb977-2slw4", UID:"f7528079-418f-4e20-90d9-6d73424a7a8a", APIVersion:"v1", ResourceVersion:"61291", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0618 14:20:12.240727       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
W0618 14:20:14.017304       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
W0618 14:20:14.017344       7 controller.go:1044] Error obtaining Endpoints for Service "audit-log/appfrontend": no object matching key "audit-log/appfrontend" in local store
I0618 14:20:14.040828       7 admission.go:149] processed ingress via admission controller {testedIngressLength:2 testedIngressTime:0.023s renderingIngressLength:2 renderingIngressTime:0s admissionTime:28.9kBs testedConfigurationSize:0.023}
I0618 14:20:14.040908       7 main.go:100] "successfully validated configuration, accepting" ingress="audit-log/frontend-ingress"
I0618 14:20:14.048484       7 store.go:433] "Found valid IngressClass" ingress="audit-log/frontend-ingress" ingressclass="nginx"
I0618 14:20:14.048810       7 event.go:285] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"audit-log", Name:"frontend-ingress", UID:"6874810d-18ee-41e6-8cd8-725a46f332b1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"62307", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W0618 14:20:15.574538       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
W0618 14:20:15.574637       7 controller.go:1151] Service "audit-log/appfrontend" does not have any active Endpoint.
I0618 14:20:15.574691       7 controller.go:189] "Configuration changes detected, backend reload required"
I0618 14:20:15.605669       7 controller.go:206] "Backend successfully reloaded"
I0618 14:20:15.606137       7 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-6cc5ccb977-2slw4", UID:"f7528079-418f-4e20-90d9-6d73424a7a8a", APIVersion:"v1", ResourceVersion:"61291", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0618 14:20:18.906919       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
W0618 14:20:22.240890       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
I0618 14:21:02.334412       7 status.go:300] "updating Ingress status" namespace="audit-log" ingress="backend-ingress" currentValue=[] newValue=[{IP:192.168.105.52 Hostname: Ports:[]}]
I0618 14:21:02.334566       7 status.go:300] "updating Ingress status" namespace="audit-log" ingress="frontend-ingress" currentValue=[] newValue=[{IP:192.168.105.52 Hostname: Ports:[]}]
I0618 14:21:02.341967       7 event.go:285] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"audit-log", Name:"backend-ingress", UID:"e5f28ca7-986e-4493-afe9-669ca07e45c2", APIVersion:"networking.k8s.io/v1", ResourceVersion:"62395", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0618 14:21:02.341992       7 event.go:285] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"audit-log", Name:"frontend-ingress", UID:"6874810d-18ee-41e6-8cd8-725a46f332b1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"62396", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0618 14:24:58.442644       7 admission.go:149] processed ingress via admission controller {testedIngressLength:2 testedIngressTime:0.015s renderingIngressLength:2 renderingIngressTime:0s admissionTime:21.5kBs testedConfigurationSize:0.015}
I0618 14:24:58.442676       7 main.go:100] "successfully validated configuration, accepting" ingress="audit-log/frontend-ingress"
I0618 14:24:58.447709       7 controller.go:189] "Configuration changes detected, backend reload required"
I0618 14:24:58.447832       7 event.go:285] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"audit-log", Name:"frontend-ingress", UID:"6874810d-18ee-41e6-8cd8-725a46f332b1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"62659", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0618 14:24:58.477891       7 controller.go:206] "Backend successfully reloaded"
I0618 14:24:58.478074       7 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-6cc5ccb977-2slw4", UID:"f7528079-418f-4e20-90d9-6d73424a7a8a", APIVersion:"v1", ResourceVersion:"61291", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0621 13:30:31.635449       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0621 13:30:31.635539       7 nginx.go:380] "Shutting down controller queues"

* 
* ==> controller_ingress [f2492dea2b8c] <==
* -------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.7.0
  Build:         72ff21ed9e26cb969052c753633049ba8a87ecf9
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------

W0621 13:30:58.693803       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0621 13:30:58.693946       7 main.go:209] "Creating API client" host="https://10.96.0.1:443"
W0621 13:31:22.026518       7 main.go:250] Initial connection to the Kubernetes API server was retried 5 times.
I0621 13:31:22.026552       7 main.go:253] "Running in Kubernetes cluster" major="1" minor="26" git="v1.26.3" state="clean" commit="9e644106593f3f4aa98f8a84b23db5fa378900bd" platform="linux/arm64"
I0621 13:31:22.551418       7 main.go:104] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0621 13:31:22.564695       7 ssl.go:533] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0621 13:31:22.573595       7 nginx.go:261] "Starting NGINX Ingress controller"
I0621 13:31:22.579896       7 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"a486f99d-01f6-4723-8d35-85180a3694c6", APIVersion:"v1", ResourceVersion:"61251", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0621 13:31:22.580029       7 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"e0acda7c-8d61-42bb-aa0e-1f2c8b6d107f", APIVersion:"v1", ResourceVersion:"61252", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0621 13:31:22.580163       7 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"a6aa8b5e-33ca-4b93-9d33-52c57d4decd5", APIVersion:"v1", ResourceVersion:"61250", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0621 13:31:23.676632       7 store.go:433] "Found valid IngressClass" ingress="audit-log/backend-ingress" ingressclass="nginx"
I0621 13:31:23.676914       7 store.go:433] "Found valid IngressClass" ingress="audit-log/frontend-ingress" ingressclass="nginx"
I0621 13:31:23.677538       7 event.go:285] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"audit-log", Name:"backend-ingress", UID:"e5f28ca7-986e-4493-afe9-669ca07e45c2", APIVersion:"networking.k8s.io/v1", ResourceVersion:"62395", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0621 13:31:23.677601       7 event.go:285] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"audit-log", Name:"frontend-ingress", UID:"6874810d-18ee-41e6-8cd8-725a46f332b1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"62659", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0621 13:31:23.775436       7 leaderelection.go:248] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0621 13:31:23.775606       7 nginx.go:304] "Starting NGINX process"
I0621 13:31:23.776676       7 nginx.go:324] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0621 13:31:23.776995       7 controller.go:189] "Configuration changes detected, backend reload required"
I0621 13:31:23.783686       7 leaderelection.go:258] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0621 13:31:23.783766       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-6cc5ccb977-2slw4"
I0621 13:31:23.836339       7 controller.go:206] "Backend successfully reloaded"
I0621 13:31:23.836468       7 controller.go:217] "Initial sync, sleeping for 1 second"
I0621 13:31:23.836664       7 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-6cc5ccb977-2slw4", UID:"f7528079-418f-4e20-90d9-6d73424a7a8a", APIVersion:"v1", ResourceVersion:"61411", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0621 13:31:30.941376       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
W0621 13:31:34.272865       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
W0621 13:31:34.272925       7 controller.go:1151] Service "audit-log/appfrontend" does not have any active Endpoint.
W0621 13:31:37.606241       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
W0621 13:31:37.606315       7 controller.go:1151] Service "audit-log/appfrontend" does not have any active Endpoint.
W0621 13:31:40.938722       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.
W0621 13:31:44.274076       7 controller.go:1151] Service "audit-log/appbackend" does not have any active Endpoint.

* 
* ==> coredns [06142eed5119] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = ea7a0d73d9d208f758b1f67640ef03c58089b9d9366cf3478df3bb369b210e39f213811b46224f8a04380814b6e0890ccd358f5b5e8c80bc22ac19c8601ee35b
CoreDNS-1.9.3
linux/arm64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:34701 - 41007 "HINFO IN 2302755501747347777.4528006233580367180. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.015962959s
[INFO] 10.244.0.155:56113 - 29876 "A IN registry.npmjs.org.audit-log.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000298417s
[INFO] 10.244.0.155:45475 - 47410 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000190292s
[INFO] 10.244.0.155:39806 - 14658 "A IN registry.npmjs.org.audit-log.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000045791s
[INFO] 10.244.0.155:56366 - 50144 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000072083s
[INFO] 10.244.0.155:38673 - 52456 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000039375s
[INFO] 10.244.0.155:54654 - 59553 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000055084s
[INFO] 10.244.0.155:40012 - 43470 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.011732488s
[INFO] 10.244.0.155:33566 - 8643 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.011244529s
[INFO] 10.244.0.156:49329 - 43257 "A IN jhipster-registry-1.jhipster-registry.audit-log.svc.cluster.local. udp 83 false 512" NOERROR qr,aa,rd 164 0.000252834s
[INFO] 10.244.0.144:40055 - 20731 "A IN jhipster-registry-0.jhipster-registry.audit-log.svc.cluster.local. udp 83 false 512" NOERROR qr,aa,rd 164 0.000283584s
[INFO] 10.244.0.144:41638 - 54737 "A IN otel-collector.audit-log.svc.cluster.local. udp 60 false 512" NOERROR qr,aa,rd 118 0.000173542s
[INFO] 10.244.0.156:54668 - 49013 "A IN otel-collector.audit-log.svc.cluster.local. udp 60 false 512" NOERROR qr,aa,rd 118 0.000099667s
[INFO] 10.244.0.155:48509 - 16818 "A IN www.google.com. udp 32 false 512" NOERROR qr,rd,ra 62 0.003657467s
[INFO] 10.244.0.144:44832 - 52296 "A IN otel-collector.audit-log.svc.cluster.local. udp 60 false 512" NOERROR qr,aa,rd 118 0.000376834s
[INFO] 10.244.0.156:60421 - 3089 "A IN otel-collector.audit-log.svc.cluster.local. udp 60 false 512" NOERROR qr,aa,rd 118 0.000087s

* 
* ==> coredns [2cd2105d3c88] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = ea7a0d73d9d208f758b1f67640ef03c58089b9d9366cf3478df3bb369b210e39f213811b46224f8a04380814b6e0890ccd358f5b5e8c80bc22ac19c8601ee35b
CoreDNS-1.9.3
linux/arm64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:56286 - 16836 "HINFO IN 8632522999082586457.5910093363855384800. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.018135088s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               cluster-2
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=cluster-2
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=08896fd1dc362c097c925146c4a0d0dac715ace0
                    minikube.k8s.io/name=cluster-2
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_06_09T16_43_19_0700
                    minikube.k8s.io/version=v1.30.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 09 Jun 2023 13:43:16 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  cluster-2
  AcquireTime:     <unset>
  RenewTime:       Wed, 21 Jun 2023 13:40:08 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 21 Jun 2023 13:36:23 +0000   Fri, 09 Jun 2023 13:43:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 21 Jun 2023 13:36:23 +0000   Fri, 09 Jun 2023 13:43:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 21 Jun 2023 13:36:23 +0000   Fri, 09 Jun 2023 13:43:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 21 Jun 2023 13:36:23 +0000   Sun, 18 Jun 2023 12:50:53 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.105.52
  Hostname:    cluster-2
Capacity:
  cpu:                6
  ephemeral-storage:  17784760Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             12226280Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  17784760Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             12226280Ki
  pods:               110
System Info:
  Machine ID:                    0d5e90be391149cf8bebaacb99a21f32
  System UUID:                   0d5e90be391149cf8bebaacb99a21f32
  Boot ID:                       f680c785-3795-4b6c-a6a5-547f2bf2507c
  Kernel Version:                5.10.57
  OS Image:                      Buildroot 2021.02.12
  Operating System:              linux
  Architecture:                  arm64
  Container Runtime Version:     docker://20.10.23
  Kubelet Version:               v1.26.3
  Kube-Proxy Version:            v1.26.3
PodCIDR:                         10.244.0.0/24
PodCIDRs:                        10.244.0.0/24
Non-terminated Pods:             (23 in total)
  Namespace                      Name                                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                      ----                                                          ------------  ----------  ---------------  -------------  ---
  audit-log                      appbackend-98c79665c-57qbh                                    500m (8%!)(MISSING)     1 (16%!)(MISSING)     512Mi (4%!)(MISSING)       1Gi (8%!)(MISSING)       2d23h
  audit-log                      appbackend-postgresql-6cdd8d47ff-ljtrv                        500m (8%!)(MISSING)     1 (16%!)(MISSING)     512Mi (4%!)(MISSING)       1Gi (8%!)(MISSING)       2d23h
  audit-log                      appfrontend-75bd6f9fc5-qg7xt                                  500m (8%!)(MISSING)     1 (16%!)(MISSING)     512Mi (4%!)(MISSING)       2Gi (17%!)(MISSING)      2d23h
  audit-log                      jhipster-registry-0                                           50m (0%!)(MISSING)      500m (8%!)(MISSING)   64Mi (0%!)(MISSING)        64Mi (0%!)(MISSING)      3d
  audit-log                      jhipster-registry-1                                           50m (0%!)(MISSING)      500m (8%!)(MISSING)   64Mi (0%!)(MISSING)        64Mi (0%!)(MISSING)      3d
  audit-log                      otel-collector-7565456ffd-zrqxd                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  audit-log                      simplest-collector-6dc7464dc8-zspw8                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d
  cert-manager                   cert-manager-6ffb79dfdb-z7wwg                                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  cert-manager                   cert-manager-cainjector-5fcd49c96-s5lqw                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  cert-manager                   cert-manager-webhook-796ff7697b-bgfnw                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  default                        jaeger-75557dfd79-ctfrk                                       50m (0%!)(MISSING)      500m (8%!)(MISSING)   64Mi (0%!)(MISSING)        64Mi (0%!)(MISSING)      2d22h
  default                        jaeger-operator-559c54b76c-8fvb2                              50m (0%!)(MISSING)      500m (8%!)(MISSING)   64Mi (0%!)(MISSING)        64Mi (0%!)(MISSING)      2d22h
  default                        otel-collector-84ffffc47b-8lk78                               50m (0%!)(MISSING)      500m (8%!)(MISSING)   64Mi (0%!)(MISSING)        64Mi (0%!)(MISSING)      3d
  ingress-nginx                  ingress-nginx-controller-6cc5ccb977-2slw4                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         2d23h
  kube-system                    coredns-787d4945fb-kc7lm                                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     11d
  kube-system                    etcd-cluster-2                                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         11d
  kube-system                    kube-apiserver-cluster-2                                      250m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kube-system                    kube-controller-manager-cluster-2                             200m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kube-system                    kube-proxy-sgd4c                                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kube-system                    kube-scheduler-cluster-2                                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kube-system                    storage-provisioner                                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  observability                  jaeger-operator-7dbc884b94-tkl96                              105m (1%!)(MISSING)     1 (16%!)(MISSING)     192Mi (1%!)(MISSING)       640Mi (5%!)(MISSING)     2d22h
  opentelemetry-operator-system  opentelemetry-operator-controller-manager-75c8ffdf67-4t4w8    105m (1%!)(MISSING)     500m (8%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     8d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                2810m (46%!)(MISSING)   7 (116%!)(MISSING)
  memory             2436Mi (20%!)(MISSING)  5354Mi (44%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 8m46s                  kube-proxy       
  Normal  Starting                 8m58s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  8m58s (x8 over 8m58s)  kubelet          Node cluster-2 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8m58s (x8 over 8m58s)  kubelet          Node cluster-2 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8m58s (x7 over 8m58s)  kubelet          Node cluster-2 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  8m58s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           8m41s                  node-controller  Node cluster-2 event: Registered Node cluster-2 in Controller

* 
* ==> dmesg <==
* [  +2.228738] systemd-fstab-generator[1235]: Ignoring "noauto" for root device
[  +0.191312] systemd-fstab-generator[1273]: Ignoring "noauto" for root device
[  +0.075728] systemd-fstab-generator[1284]: Ignoring "noauto" for root device
[  +0.095959] systemd-fstab-generator[1297]: Ignoring "noauto" for root device
[  +1.251781] kauditd_printk_skb: 30 callbacks suppressed
[  +0.148146] systemd-fstab-generator[1486]: Ignoring "noauto" for root device
[  +0.076748] systemd-fstab-generator[1497]: Ignoring "noauto" for root device
[  +0.071744] systemd-fstab-generator[1508]: Ignoring "noauto" for root device
[  +0.075893] systemd-fstab-generator[1519]: Ignoring "noauto" for root device
[  +0.095928] systemd-fstab-generator[1551]: Ignoring "noauto" for root device
[Jun20 16:41] systemd-fstab-generator[1828]: Ignoring "noauto" for root device
[  +0.324179] kauditd_printk_skb: 29 callbacks suppressed
[  +7.603215] kauditd_printk_skb: 10 callbacks suppressed
[  +5.249639] kauditd_printk_skb: 64 callbacks suppressed
[  +5.620859] kauditd_printk_skb: 89 callbacks suppressed
[  +5.399491] kauditd_printk_skb: 2 callbacks suppressed
[Jun20 16:50] kauditd_printk_skb: 36 callbacks suppressed
[ +11.480265] kauditd_printk_skb: 11 callbacks suppressed
[Jun20 16:52] kauditd_printk_skb: 11 callbacks suppressed
[ +24.948220] kauditd_printk_skb: 13 callbacks suppressed
[Jun20 16:54] kauditd_printk_skb: 11 callbacks suppressed
[ +20.618852] kauditd_printk_skb: 13 callbacks suppressed
[Jun20 16:56] kauditd_printk_skb: 13 callbacks suppressed
[Jun20 16:58] kauditd_printk_skb: 13 callbacks suppressed
[  +8.899057] kauditd_printk_skb: 11 callbacks suppressed
[Jun20 17:16] kauditd_printk_skb: 9 callbacks suppressed
[Jun20 17:17] kauditd_printk_skb: 3 callbacks suppressed
[Jun20 17:19] kauditd_printk_skb: 10 callbacks suppressed
[Jun20 17:58] kauditd_printk_skb: 3 callbacks suppressed
[Jun20 18:00] kauditd_printk_skb: 4 callbacks suppressed
[Jun20 18:01] kauditd_printk_skb: 27 callbacks suppressed
[Jun20 18:02] kauditd_printk_skb: 3 callbacks suppressed
[ +21.773135] kauditd_printk_skb: 2 callbacks suppressed
[  +5.604205] kauditd_printk_skb: 24 callbacks suppressed
[Jun20 18:10] kauditd_printk_skb: 10 callbacks suppressed
[ +27.683676] kauditd_printk_skb: 10 callbacks suppressed
[ +22.209664] kauditd_printk_skb: 13 callbacks suppressed
[Jun20 18:35] kauditd_printk_skb: 4 callbacks suppressed
[ +13.113114] kauditd_printk_skb: 3 callbacks suppressed
[Jun20 18:41] kauditd_printk_skb: 3 callbacks suppressed
[  +8.043225] kauditd_printk_skb: 11 callbacks suppressed
[Jun20 18:58] kauditd_printk_skb: 3 callbacks suppressed
[Jun20 19:32] hrtimer: interrupt took 24909623 ns
[Jun21 13:30] systemd-fstab-generator[2578053]: Ignoring "noauto" for root device
[  +0.279712] systemd-fstab-generator[2578090]: Ignoring "noauto" for root device
[  +0.225287] systemd-fstab-generator[2578102]: Ignoring "noauto" for root device
[  +0.217551] systemd-fstab-generator[2578115]: Ignoring "noauto" for root device
[  +5.814116] kauditd_printk_skb: 73 callbacks suppressed
[ +16.188622] kauditd_printk_skb: 32 callbacks suppressed
[  +0.970314] systemd-fstab-generator[2582590]: Ignoring "noauto" for root device
[  +0.174463] systemd-fstab-generator[2582996]: Ignoring "noauto" for root device
[  +0.180274] systemd-fstab-generator[2583350]: Ignoring "noauto" for root device
[  +0.240132] systemd-fstab-generator[2583634]: Ignoring "noauto" for root device
[  +0.287124] systemd-fstab-generator[2583972]: Ignoring "noauto" for root device
[  +3.154118] kauditd_printk_skb: 298 callbacks suppressed
[Jun21 13:31] kauditd_printk_skb: 22 callbacks suppressed
[  +1.108400] systemd-fstab-generator[2590589]: Ignoring "noauto" for root device
[ +22.731607] kauditd_printk_skb: 2 callbacks suppressed
[  +5.662187] kauditd_printk_skb: 2 callbacks suppressed
[Jun21 13:32] kauditd_printk_skb: 14 callbacks suppressed

* 
* ==> etcd [94b8514a3ab5] <==
* {"level":"warn","ts":"2023-06-21T13:30:56.557Z","caller":"flags/flag.go:93","msg":"unrecognized environment variable","environment-variable":"ETCD_UNSUPPORTED_ARCH=arm64"}
{"level":"info","ts":"2023-06-21T13:30:56.558Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.105.52:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.105.52:2380","--initial-cluster=cluster-2=https://192.168.105.52:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.105.52:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.105.52:2380","--name=cluster-2","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-06-21T13:30:56.558Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-06-21T13:30:56.558Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.105.52:2380"]}
{"level":"info","ts":"2023-06-21T13:30:56.558Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-06-21T13:30:56.558Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.52:2379"]}
{"level":"info","ts":"2023-06-21T13:30:56.558Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.6","git-sha":"cecbe35ce","go-version":"go1.17.13","go-os":"linux","go-arch":"arm64","max-cpu-set":6,"max-cpu-available":6,"member-initialized":true,"name":"cluster-2","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.105.52:2380"],"listen-peer-urls":["https://192.168.105.52:2380"],"advertise-client-urls":["https://192.168.105.52:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.52:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-06-21T13:30:56.568Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.692171ms"}
{"level":"info","ts":"2023-06-21T13:30:56.941Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":160017,"snapshot-size":"9.7 kB"}
{"level":"info","ts":"2023-06-21T13:30:56.942Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":10178560,"backend-size":"10 MB","backend-size-in-use-bytes":3698688,"backend-size-in-use":"3.7 MB"}
{"level":"info","ts":"2023-06-21T13:30:57.164Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"e60d9e0a2267b88a","local-member-id":"b58dcdf76290e997","commit-index":168622}
{"level":"info","ts":"2023-06-21T13:30:57.164Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 switched to configuration voters=(13082338954975439255)"}
{"level":"info","ts":"2023-06-21T13:30:57.164Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 became follower at term 10"}
{"level":"info","ts":"2023-06-21T13:30:57.164Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b58dcdf76290e997 [peers: [b58dcdf76290e997], term: 10, commit: 168622, applied: 160017, lastindex: 168622, lastterm: 10]"}
{"level":"info","ts":"2023-06-21T13:30:57.164Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-06-21T13:30:57.164Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"e60d9e0a2267b88a","local-member-id":"b58dcdf76290e997","recovered-remote-peer-id":"b58dcdf76290e997","recovered-remote-peer-urls":["https://192.168.105.52:2380"]}
{"level":"info","ts":"2023-06-21T13:30:57.164Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-06-21T13:30:57.166Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-06-21T13:30:57.166Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":141512}
{"level":"info","ts":"2023-06-21T13:30:57.169Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":142185}
{"level":"info","ts":"2023-06-21T13:30:57.170Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-06-21T13:30:57.172Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"b58dcdf76290e997","timeout":"7s"}
{"level":"info","ts":"2023-06-21T13:30:57.172Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"b58dcdf76290e997"}
{"level":"info","ts":"2023-06-21T13:30:57.172Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"b58dcdf76290e997","local-server-version":"3.5.6","cluster-id":"e60d9e0a2267b88a","cluster-version":"3.5"}
{"level":"info","ts":"2023-06-21T13:30:57.172Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b58dcdf76290e997","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-06-21T13:30:57.172Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-06-21T13:30:57.173Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-06-21T13:30:57.173Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-06-21T13:30:57.173Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-06-21T13:30:57.174Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.105.52:2380"}
{"level":"info","ts":"2023-06-21T13:30:57.174Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.105.52:2380"}
{"level":"info","ts":"2023-06-21T13:30:57.174Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"b58dcdf76290e997","initial-advertise-peer-urls":["https://192.168.105.52:2380"],"listen-peer-urls":["https://192.168.105.52:2380"],"advertise-client-urls":["https://192.168.105.52:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.52:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-06-21T13:30:57.174Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-06-21T13:30:57.565Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 is starting a new election at term 10"}
{"level":"info","ts":"2023-06-21T13:30:57.565Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 became pre-candidate at term 10"}
{"level":"info","ts":"2023-06-21T13:30:57.565Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 received MsgPreVoteResp from b58dcdf76290e997 at term 10"}
{"level":"info","ts":"2023-06-21T13:30:57.565Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 became candidate at term 11"}
{"level":"info","ts":"2023-06-21T13:30:57.565Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 received MsgVoteResp from b58dcdf76290e997 at term 11"}
{"level":"info","ts":"2023-06-21T13:30:57.565Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 became leader at term 11"}
{"level":"info","ts":"2023-06-21T13:30:57.565Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b58dcdf76290e997 elected leader b58dcdf76290e997 at term 11"}
{"level":"info","ts":"2023-06-21T13:30:57.566Z","caller":"etcdserver/server.go:2054","msg":"published local member to cluster through raft","local-member-id":"b58dcdf76290e997","local-member-attributes":"{Name:cluster-2 ClientURLs:[https://192.168.105.52:2379]}","request-path":"/0/members/b58dcdf76290e997/attributes","cluster-id":"e60d9e0a2267b88a","publish-timeout":"7s"}
{"level":"info","ts":"2023-06-21T13:30:57.566Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-06-21T13:30:57.567Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.105.52:2379"}
{"level":"info","ts":"2023-06-21T13:30:57.567Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-06-21T13:30:57.567Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-06-21T13:30:57.574Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-06-21T13:30:57.574Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-06-21T13:31:06.123Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-06-21T13:31:06.124Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"cluster-2","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.105.52:2380"],"advertise-client-urls":["https://192.168.105.52:2379"]}
{"level":"info","ts":"2023-06-21T13:31:06.126Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"b58dcdf76290e997","current-leader-member-id":"b58dcdf76290e997"}
{"level":"info","ts":"2023-06-21T13:31:06.130Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.105.52:2380"}
{"level":"info","ts":"2023-06-21T13:31:06.131Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.105.52:2380"}
{"level":"info","ts":"2023-06-21T13:31:06.131Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"cluster-2","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.105.52:2380"],"advertise-client-urls":["https://192.168.105.52:2379"]}

* 
* ==> etcd [b7956d777588] <==
* {"level":"warn","ts":"2023-06-21T13:31:13.312Z","caller":"flags/flag.go:93","msg":"unrecognized environment variable","environment-variable":"ETCD_UNSUPPORTED_ARCH=arm64"}
{"level":"info","ts":"2023-06-21T13:31:13.314Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.105.52:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.105.52:2380","--initial-cluster=cluster-2=https://192.168.105.52:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.105.52:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.105.52:2380","--name=cluster-2","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-06-21T13:31:13.314Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-06-21T13:31:13.314Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.105.52:2380"]}
{"level":"info","ts":"2023-06-21T13:31:13.314Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-06-21T13:31:13.314Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.52:2379"]}
{"level":"info","ts":"2023-06-21T13:31:13.314Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.6","git-sha":"cecbe35ce","go-version":"go1.17.13","go-os":"linux","go-arch":"arm64","max-cpu-set":6,"max-cpu-available":6,"member-initialized":true,"name":"cluster-2","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.105.52:2380"],"listen-peer-urls":["https://192.168.105.52:2380"],"advertise-client-urls":["https://192.168.105.52:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.52:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-06-21T13:31:13.316Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.661588ms"}
{"level":"info","ts":"2023-06-21T13:31:13.598Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":160017,"snapshot-size":"9.7 kB"}
{"level":"info","ts":"2023-06-21T13:31:13.598Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":10178560,"backend-size":"10 MB","backend-size-in-use-bytes":3698688,"backend-size-in-use":"3.7 MB"}
{"level":"info","ts":"2023-06-21T13:31:13.705Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"e60d9e0a2267b88a","local-member-id":"b58dcdf76290e997","commit-index":168624}
{"level":"info","ts":"2023-06-21T13:31:13.705Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 switched to configuration voters=(13082338954975439255)"}
{"level":"info","ts":"2023-06-21T13:31:13.705Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 became follower at term 11"}
{"level":"info","ts":"2023-06-21T13:31:13.705Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b58dcdf76290e997 [peers: [b58dcdf76290e997], term: 11, commit: 168624, applied: 160017, lastindex: 168624, lastterm: 11]"}
{"level":"info","ts":"2023-06-21T13:31:13.705Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-06-21T13:31:13.705Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"e60d9e0a2267b88a","local-member-id":"b58dcdf76290e997","recovered-remote-peer-id":"b58dcdf76290e997","recovered-remote-peer-urls":["https://192.168.105.52:2380"]}
{"level":"info","ts":"2023-06-21T13:31:13.705Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-06-21T13:31:13.707Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-06-21T13:31:13.707Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":141512}
{"level":"info","ts":"2023-06-21T13:31:13.710Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":142185}
{"level":"info","ts":"2023-06-21T13:31:13.710Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-06-21T13:31:13.711Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"b58dcdf76290e997","timeout":"7s"}
{"level":"info","ts":"2023-06-21T13:31:13.712Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"b58dcdf76290e997"}
{"level":"info","ts":"2023-06-21T13:31:13.712Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"b58dcdf76290e997","local-server-version":"3.5.6","cluster-id":"e60d9e0a2267b88a","cluster-version":"3.5"}
{"level":"info","ts":"2023-06-21T13:31:13.712Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b58dcdf76290e997","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-06-21T13:31:13.712Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-06-21T13:31:13.712Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-06-21T13:31:13.712Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-06-21T13:31:13.713Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-06-21T13:31:13.713Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"b58dcdf76290e997","initial-advertise-peer-urls":["https://192.168.105.52:2380"],"listen-peer-urls":["https://192.168.105.52:2380"],"advertise-client-urls":["https://192.168.105.52:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.105.52:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-06-21T13:31:13.713Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-06-21T13:31:13.713Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.105.52:2380"}
{"level":"info","ts":"2023-06-21T13:31:13.713Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.105.52:2380"}
{"level":"info","ts":"2023-06-21T13:31:14.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 is starting a new election at term 11"}
{"level":"info","ts":"2023-06-21T13:31:14.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 became pre-candidate at term 11"}
{"level":"info","ts":"2023-06-21T13:31:14.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 received MsgPreVoteResp from b58dcdf76290e997 at term 11"}
{"level":"info","ts":"2023-06-21T13:31:14.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 became candidate at term 12"}
{"level":"info","ts":"2023-06-21T13:31:14.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 received MsgVoteResp from b58dcdf76290e997 at term 12"}
{"level":"info","ts":"2023-06-21T13:31:14.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b58dcdf76290e997 became leader at term 12"}
{"level":"info","ts":"2023-06-21T13:31:14.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b58dcdf76290e997 elected leader b58dcdf76290e997 at term 12"}
{"level":"info","ts":"2023-06-21T13:31:14.009Z","caller":"etcdserver/server.go:2054","msg":"published local member to cluster through raft","local-member-id":"b58dcdf76290e997","local-member-attributes":"{Name:cluster-2 ClientURLs:[https://192.168.105.52:2379]}","request-path":"/0/members/b58dcdf76290e997/attributes","cluster-id":"e60d9e0a2267b88a","publish-timeout":"7s"}
{"level":"info","ts":"2023-06-21T13:31:14.009Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-06-21T13:31:14.009Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-06-21T13:31:14.010Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-06-21T13:31:14.010Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-06-21T13:31:14.010Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.105.52:2379"}
{"level":"info","ts":"2023-06-21T13:31:14.010Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}

* 
* ==> kernel <==
*  13:40:11 up 20:59,  0 users,  load average: 0.58, 0.94, 0.96
Linux cluster-2 5.10.57 #1 SMP PREEMPT Mon Apr 3 22:26:25 UTC 2023 aarch64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-apiserver [a2fc7c591bcd] <==
* I0621 13:31:16.767777       1 controller.go:85] Starting OpenAPI controller
E0621 13:31:16.774265       1 controller.go:159] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0621 13:31:16.780542       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0621 13:31:16.788511       1 shared_informer.go:280] Caches are synced for node_authorizer
I0621 13:31:16.842681       1 shared_informer.go:280] Caches are synced for cluster_authentication_trust_controller
I0621 13:31:16.842709       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0621 13:31:16.842680       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0621 13:31:16.842725       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0621 13:31:16.842688       1 shared_informer.go:280] Caches are synced for configmaps
I0621 13:31:16.842953       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0621 13:31:16.842961       1 cache.go:39] Caches are synced for autoregister controller
I0621 13:31:16.843036       1 shared_informer.go:280] Caches are synced for crd-autoregister
I0621 13:31:17.590805       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0621 13:31:17.745741       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0621 13:31:18.377979       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0621 13:31:28.383807       1 trace.go:219] Trace[973158822]: "Create" accept:application/json, */*,audit-id:b18303de-98b0-43e5-a3f6-cbb3c9aad73d,client:192.168.105.52,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments,user-agent:kubeadm/v1.26.3 (linux/arm64) kubernetes/9e64410,verb:POST (21-Jun-2023 13:31:18.382) (total time: 10001ms):
Trace[973158822]: ["Call mutating webhook" configuration:jaeger-operator-mutating-webhook-configuration,webhook:deployment.sidecar-injector.jaegertracing.io,resource:apps/v1, Resource=deployments,subresource:,operation:CREATE,UID:ceac2ccf-8010-4f2c-92c6-6e8591e81a13 9999ms (13:31:18.384)]
Trace[973158822]: [10.001190593s] [10.001190593s] END
W0621 13:31:28.384284       1 dispatcher.go:181] Failed calling webhook, failing open deployment.sidecar-injector.jaegertracing.io: failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: i/o timeout
E0621 13:31:28.384382       1 dispatcher.go:185] failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: i/o timeout
E0621 13:31:28.384481       1 timeout.go:142] post-timeout activity - time-elapsed: 236.542µs, POST "/apis/apps/v1/namespaces/kube-system/deployments" result: <nil>
E0621 13:31:28.384232       1 wrap.go:54] timeout or abort while handling: method=POST URI="/apis/apps/v1/namespaces/kube-system/deployments?timeout=10s" audit-ID="b18303de-98b0-43e5-a3f6-cbb3c9aad73d"
E0621 13:31:28.384583       1 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 332.834µs, panicked: false, err: Timeout: request did not complete within requested timeout, panic-reason: <nil>
I0621 13:31:29.055060       1 controller.go:615] quota admission added evaluator for: endpoints
I0621 13:31:29.404205       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
W0621 13:31:30.029707       1 dispatcher.go:181] Failed calling webhook, failing open deployment.sidecar-injector.jaegertracing.io: failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: connect: no route to host
E0621 13:31:30.029884       1 dispatcher.go:185] failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: connect: no route to host
W0621 13:31:30.029785       1 dispatcher.go:181] Failed calling webhook, failing open deployment.sidecar-injector.jaegertracing.io: failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: connect: no route to host
E0621 13:31:30.030175       1 dispatcher.go:185] failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: connect: no route to host
I0621 13:31:30.030889       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0621 13:31:30.032055       1 trace.go:219] Trace[2042727792]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:162ae507-171f-4cae-a345-193fef7d3ee6,client:10.244.0.148,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/default/deployments/jaeger,user-agent:jaeger-operator/v0.0.0 (linux/arm64) kubernetes/$Format,verb:PUT (21-Jun-2023 13:31:25.946) (total time: 4085ms):
Trace[2042727792]: ["GuaranteedUpdate etcd3" audit-id:162ae507-171f-4cae-a345-193fef7d3ee6,key:/deployments/default/jaeger,type:*apps.Deployment,resource:deployments.apps 4084ms (13:31:25.947)
Trace[2042727792]:  ---"About to Encode" 4083ms (13:31:30.030)]
Trace[2042727792]: ["Call mutating webhook" configuration:jaeger-operator-mutating-webhook-configuration,webhook:deployment.sidecar-injector.jaegertracing.io,resource:apps/v1, Resource=deployments,subresource:,operation:UPDATE,UID:3cca77f8-5a29-49e6-bf34-b6104fb226e5 4083ms (13:31:25.948)]
Trace[2042727792]: [4.085611174s] [4.085611174s] END
I0621 13:31:30.038044       1 trace.go:219] Trace[551018687]: "Create" accept:application/json, */*,audit-id:98c36df9-c363-4b40-9b0b-2c86cf4234a8,client:192.168.105.52,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments,user-agent:kubeadm/v1.26.3 (linux/arm64) kubernetes/9e64410,verb:POST (21-Jun-2023 13:31:28.521) (total time: 1516ms):
Trace[551018687]: ["Call mutating webhook" configuration:jaeger-operator-mutating-webhook-configuration,webhook:deployment.sidecar-injector.jaegertracing.io,resource:apps/v1, Resource=deployments,subresource:,operation:CREATE,UID:aeca34e6-d68b-42fa-b018-4a5fc341a63c 1515ms (13:31:28.522)]
Trace[551018687]: [1.516347889s] [1.516347889s] END
W0621 13:31:33.103763       1 dispatcher.go:181] Failed calling webhook, failing open deployment.sidecar-injector.jaegertracing.io: failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: connect: no route to host
E0621 13:31:33.103909       1 dispatcher.go:185] failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: connect: no route to host
W0621 13:31:33.111958       1 dispatcher.go:181] Failed calling webhook, failing open deployment.sidecar-injector.jaegertracing.io: failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=7s": dial tcp 10.99.247.67:443: connect: connection refused
E0621 13:31:33.111991       1 dispatcher.go:185] failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=7s": dial tcp 10.99.247.67:443: connect: connection refused
I0621 13:31:33.117721       1 trace.go:219] Trace[233529623]: "Update" accept:application/json, */*,audit-id:aa3055ba-551b-465f-b029-04ffd4c4f92f,client:192.168.105.52,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/kube-system/deployments/coredns,user-agent:kubeadm/v1.26.3 (linux/arm64) kubernetes/9e64410,verb:PUT (21-Jun-2023 13:31:30.039) (total time: 3078ms):
Trace[233529623]: ["GuaranteedUpdate etcd3" audit-id:aa3055ba-551b-465f-b029-04ffd4c4f92f,key:/deployments/kube-system/coredns,type:*apps.Deployment,resource:deployments.apps 3078ms (13:31:30.039)
Trace[233529623]:  ---"About to Encode" 3064ms (13:31:33.104)]
Trace[233529623]: ["Call mutating webhook" configuration:jaeger-operator-mutating-webhook-configuration,webhook:deployment.sidecar-injector.jaegertracing.io,resource:apps/v1, Resource=deployments,subresource:,operation:UPDATE,UID:7ac69259-17dd-4e6c-89ea-6ad70a3c572c 3076ms (13:31:30.041)]
Trace[233529623]: [3.078608731s] [3.078608731s] END
I0621 13:31:33.146362       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0621 13:31:33.169888       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0621 13:31:33.178541       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
W0621 13:31:34.523670       1 dispatcher.go:181] Failed calling webhook, failing open deployment.sidecar-injector.jaegertracing.io: failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: connect: connection refused
E0621 13:31:34.523713       1 dispatcher.go:185] failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": dial tcp 10.99.247.67:443: connect: connection refused
I0621 13:33:59.733834       1 controller.go:615] quota admission added evaluator for: instrumentations.opentelemetry.io
W0621 13:34:09.754042       1 dispatcher.go:181] Failed calling webhook, failing open deployment.sidecar-injector.jaegertracing.io: failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": context deadline exceeded
E0621 13:34:09.754278       1 dispatcher.go:185] failed calling webhook "deployment.sidecar-injector.jaegertracing.io": failed to call webhook: Post "https://jaeger-operator-webhook-service.default.svc:443/mutate-v1-deployment?timeout=10s": context deadline exceeded
I0621 13:34:09.756486       1 trace.go:219] Trace[706468349]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:12f1885f-6bfe-4b43-95de-27df69688019,client:10.244.0.149,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/default/deployments/otel-collector,user-agent:Go-http-client/2.0,verb:PATCH (21-Jun-2023 13:33:59.751) (total time: 10005ms):
Trace[706468349]: ["GuaranteedUpdate etcd3" audit-id:12f1885f-6bfe-4b43-95de-27df69688019,key:/deployments/default/otel-collector,type:*apps.Deployment,resource:deployments.apps 10005ms (13:33:59.751)
Trace[706468349]:  ---"About to Encode" 10003ms (13:34:09.754)]
Trace[706468349]: ["Call mutating webhook" configuration:jaeger-operator-mutating-webhook-configuration,webhook:deployment.sidecar-injector.jaegertracing.io,resource:apps/v1, Resource=deployments,subresource:,operation:UPDATE,UID:06aa07d6-1e32-479c-b919-12a4379f7512 10002ms (13:33:59.753)]
Trace[706468349]: [10.005242845s] [10.005242845s] END

* 
* ==> kube-apiserver [f8365cda492c] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 13:30:52.687087       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 13:30:52.697802       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 13:30:52.723481       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 13:30:52.753882       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 13:30:52.795015       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 13:30:52.807408       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0621 13:30:52.836446       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [991d8f2aacde] <==
* I0621 13:31:28.907001       1 controllermanager.go:622] Started "ttl-after-finished"
I0621 13:31:28.907192       1 ttlafterfinished_controller.go:104] Starting TTL after finished controller
I0621 13:31:28.907749       1 shared_informer.go:273] Waiting for caches to sync for TTL after finished
I0621 13:31:28.908927       1 controllermanager.go:622] Started "serviceaccount"
I0621 13:31:28.909127       1 serviceaccounts_controller.go:111] Starting service account controller
I0621 13:31:28.909305       1 shared_informer.go:273] Waiting for caches to sync for service account
I0621 13:31:28.919944       1 shared_informer.go:273] Waiting for caches to sync for resource quota
W0621 13:31:28.941696       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="cluster-2" does not exist
I0621 13:31:28.949179       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0621 13:31:28.949520       1 job_controller.go:514] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0621 13:31:28.955506       1 shared_informer.go:280] Caches are synced for deployment
I0621 13:31:28.961274       1 shared_informer.go:280] Caches are synced for TTL
I0621 13:31:28.961438       1 shared_informer.go:280] Caches are synced for expand
I0621 13:31:28.962529       1 shared_informer.go:280] Caches are synced for crt configmap
I0621 13:31:28.967643       1 shared_informer.go:280] Caches are synced for ReplicationController
I0621 13:31:28.967766       1 shared_informer.go:280] Caches are synced for GC
I0621 13:31:28.967779       1 shared_informer.go:280] Caches are synced for PVC protection
I0621 13:31:28.973873       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-serving
I0621 13:31:28.974054       1 shared_informer.go:280] Caches are synced for disruption
I0621 13:31:28.974118       1 shared_informer.go:280] Caches are synced for attach detach
I0621 13:31:28.977249       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-legacy-unknown
I0621 13:31:28.977339       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0621 13:31:28.977471       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-client
I0621 13:31:28.980024       1 shared_informer.go:280] Caches are synced for PV protection
I0621 13:31:28.980421       1 shared_informer.go:280] Caches are synced for ReplicaSet
I0621 13:31:28.980040       1 shared_informer.go:280] Caches are synced for ClusterRoleAggregator
I0621 13:31:28.993256       1 shared_informer.go:280] Caches are synced for ephemeral
I0621 13:31:28.995425       1 shared_informer.go:280] Caches are synced for endpoint
I0621 13:31:28.995686       1 shared_informer.go:280] Caches are synced for bootstrap_signer
I0621 13:31:28.993380       1 shared_informer.go:280] Caches are synced for persistent volume
I0621 13:31:29.011494       1 shared_informer.go:280] Caches are synced for TTL after finished
I0621 13:31:29.001279       1 shared_informer.go:273] Waiting for caches to sync for garbage collector
I0621 13:31:29.011418       1 shared_informer.go:280] Caches are synced for node
I0621 13:31:29.014194       1 range_allocator.go:167] Sending events to api server.
I0621 13:31:29.014491       1 range_allocator.go:171] Starting range CIDR allocator
I0621 13:31:29.017308       1 shared_informer.go:273] Waiting for caches to sync for cidrallocator
I0621 13:31:29.017446       1 shared_informer.go:280] Caches are synced for cidrallocator
I0621 13:31:29.017509       1 shared_informer.go:280] Caches are synced for service account
I0621 13:31:29.016483       1 shared_informer.go:280] Caches are synced for daemon sets
I0621 13:31:29.023236       1 shared_informer.go:280] Caches are synced for namespace
I0621 13:31:29.024262       1 shared_informer.go:280] Caches are synced for taint
I0621 13:31:29.027777       1 node_lifecycle_controller.go:1438] Initializing eviction metric for zone: 
W0621 13:31:29.027860       1 node_lifecycle_controller.go:1053] Missing timestamp for Node cluster-2. Assuming now as a timestamp.
I0621 13:31:29.027902       1 node_lifecycle_controller.go:1254] Controller detected that zone  is now in state Normal.
I0621 13:31:29.024583       1 shared_informer.go:280] Caches are synced for HPA
I0621 13:31:29.028176       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0621 13:31:29.028228       1 taint_manager.go:211] "Sending events to api server"
I0621 13:31:29.028506       1 event.go:294] "Event occurred" object="cluster-2" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node cluster-2 event: Registered Node cluster-2 in Controller"
I0621 13:31:29.024976       1 shared_informer.go:280] Caches are synced for stateful set
I0621 13:31:29.030335       1 shared_informer.go:280] Caches are synced for certificate-csrapproving
I0621 13:31:29.043974       1 shared_informer.go:280] Caches are synced for job
I0621 13:31:29.080929       1 event.go:294] "Event occurred" object="audit-log/jhipster-registry" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint audit-log/jhipster-registry: Operation cannot be fulfilled on endpoints \"jhipster-registry\": the object has been modified; please apply your changes to the latest version and try again"
I0621 13:31:29.085442       1 shared_informer.go:280] Caches are synced for cronjob
I0621 13:31:29.133370       1 shared_informer.go:280] Caches are synced for endpoint_slice_mirroring
I0621 13:31:29.161472       1 shared_informer.go:280] Caches are synced for endpoint_slice
I0621 13:31:29.233313       1 shared_informer.go:280] Caches are synced for resource quota
I0621 13:31:29.277386       1 shared_informer.go:280] Caches are synced for resource quota
I0621 13:31:29.516560       1 shared_informer.go:280] Caches are synced for garbage collector
I0621 13:31:29.545290       1 shared_informer.go:280] Caches are synced for garbage collector
I0621 13:31:29.545445       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage

* 
* ==> kube-controller-manager [beb8c4e1cf0a] <==
* I0621 13:30:57.375906       1 serving.go:348] Generated self-signed cert in-memory
I0621 13:30:57.863469       1 controllermanager.go:182] Version: v1.26.3
I0621 13:30:57.863488       1 controllermanager.go:184] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 13:30:57.864501       1 secure_serving.go:210] Serving securely on 127.0.0.1:10257
I0621 13:30:57.864899       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0621 13:30:57.864943       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0621 13:30:57.864962       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"

* 
* ==> kube-proxy [478fef9872d0] <==
* E0621 13:30:56.040924       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/cluster-2": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:30:57.193583       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/cluster-2": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:30:59.540494       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/cluster-2": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:03.881630       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/cluster-2": dial tcp 192.168.105.52:8443: connect: connection refused

* 
* ==> kube-proxy [5ebe08dcf047] <==
* I0621 13:31:24.617865       1 node.go:163] Successfully retrieved node IP: 192.168.105.52
I0621 13:31:24.617937       1 server_others.go:109] "Detected node IP" address="192.168.105.52"
I0621 13:31:24.617993       1 server_others.go:535] "Using iptables proxy"
I0621 13:31:24.633707       1 server_others.go:170] "kube-proxy running in single-stack mode, this ipFamily is not supported" ipFamily=IPv6
I0621 13:31:24.633733       1 server_others.go:176] "Using iptables Proxier"
I0621 13:31:24.633775       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0621 13:31:24.633997       1 server.go:655] "Version info" version="v1.26.3"
I0621 13:31:24.634008       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 13:31:24.634565       1 config.go:317] "Starting service config controller"
I0621 13:31:24.634578       1 shared_informer.go:273] Waiting for caches to sync for service config
I0621 13:31:24.634565       1 config.go:444] "Starting node config controller"
I0621 13:31:24.634589       1 shared_informer.go:273] Waiting for caches to sync for node config
I0621 13:31:24.634623       1 config.go:226] "Starting endpoint slice config controller"
I0621 13:31:24.634628       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I0621 13:31:24.734960       1 shared_informer.go:280] Caches are synced for node config
I0621 13:31:24.734996       1 shared_informer.go:280] Caches are synced for endpoint slice config
I0621 13:31:24.734979       1 shared_informer.go:280] Caches are synced for service config

* 
* ==> kube-scheduler [44b2c8cfc66c] <==
* I0621 13:31:13.645529       1 serving.go:348] Generated self-signed cert in-memory
W0621 13:31:16.778899       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0621 13:31:16.780732       1 authentication.go:349] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found, role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found]
W0621 13:31:16.780927       1 authentication.go:350] Continuing without authentication configuration. This may treat all requests as anonymous.
W0621 13:31:16.780967       1 authentication.go:351] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0621 13:31:16.788720       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.3"
I0621 13:31:16.788830       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 13:31:16.789947       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0621 13:31:16.790175       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0621 13:31:16.790219       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0621 13:31:16.790237       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0621 13:31:16.890534       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [fff4b256c8fc] <==
* E0621 13:30:58.751336       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.105.52:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:30:58.791133       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.105.52:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:30:58.791175       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.105.52:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:30:59.121397       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.105.52:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:30:59.121462       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.105.52:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:30:59.129147       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.105.52:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:30:59.129268       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.105.52:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:30:59.173661       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.105.52:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:30:59.173719       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.105.52:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:30:59.177434       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.105.52:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:30:59.177502       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.105.52:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:00.299378       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:00.299427       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:00.310041       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:00.310078       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:00.474879       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.105.52:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:00.474921       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.105.52:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:00.806775       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.105.52:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:00.806819       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.105.52:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:00.905432       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.105.52:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:00.905473       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.105.52:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:00.914732       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.105.52:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:00.914773       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.105.52:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:00.972927       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.105.52:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:00.972968       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.105.52:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:01.072244       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.105.52:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:01.072321       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.105.52:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:01.223501       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.105.52:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:01.223533       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.105.52:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:01.298030       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.105.52:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:01.298066       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.105.52:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:01.409495       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.105.52:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:01.409540       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.105.52:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:01.439667       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:01.439703       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:01.449479       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.105.52:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:01.449509       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.105.52:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:01.544366       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:01.544411       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:02.254663       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.105.52:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:02.254706       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.105.52:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:04.459124       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.105.52:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:04.459159       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.105.52:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:04.829750       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:04.829789       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:04.838333       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.105.52:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:04.838366       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.105.52:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:04.922505       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:04.922544       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:05.011397       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.105.52:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:05.011431       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.105.52:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:05.079976       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.105.52:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:05.080021       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.105.52:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:05.178555       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.105.52:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:05.178595       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.105.52:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:06.112629       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:06.112673       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
W0621 13:31:06.118397       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:06.118420       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.105.52:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.105.52:8443: connect: connection refused
E0621 13:31:06.140720       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* -- Journal begins at Sun 2023-06-18 12:50:24 UTC, ends at Wed 2023-06-21 13:40:11 UTC. --
Jun 21 13:37:38 cluster-2 kubelet[2590595]: E0621 13:37:38.906771 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": context deadline exceeded\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:37:48 cluster-2 kubelet[2590595]: I0621 13:37:48.677550 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:37:48 cluster-2 kubelet[2590595]: E0621 13:37:48.679541 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:37:53 cluster-2 kubelet[2590595]: I0621 13:37:53.676353 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:37:53 cluster-2 kubelet[2590595]: E0621 13:37:53.678687 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:38:03 cluster-2 kubelet[2590595]: I0621 13:38:03.676433 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:38:03 cluster-2 kubelet[2590595]: E0621 13:38:03.678117 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:38:07 cluster-2 kubelet[2590595]: I0621 13:38:07.676946 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:38:07 cluster-2 kubelet[2590595]: E0621 13:38:07.679630 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:38:15 cluster-2 kubelet[2590595]: I0621 13:38:15.676592 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:38:15 cluster-2 kubelet[2590595]: E0621 13:38:15.678366 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:38:19 cluster-2 kubelet[2590595]: I0621 13:38:19.676491 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:38:19 cluster-2 kubelet[2590595]: E0621 13:38:19.678801 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:38:27 cluster-2 kubelet[2590595]: I0621 13:38:27.676294 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:38:29 cluster-2 kubelet[2590595]: I0621 13:38:29.675911 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appfrontend-75bd6f9fc5-qg7xt" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:38:30 cluster-2 kubelet[2590595]: I0621 13:38:30.677728 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:38:30 cluster-2 kubelet[2590595]: E0621 13:38:30.680740 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:38:41 cluster-2 kubelet[2590595]: I0621 13:38:41.676494 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:38:41 cluster-2 kubelet[2590595]: E0621 13:38:41.678767 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:38:42 cluster-2 kubelet[2590595]: E0621 13:38:42.698583 2590595 remote_image.go:171] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp 44.205.64.79:443: i/o timeout" image="busybox:latest"
Jun 21 13:38:42 cluster-2 kubelet[2590595]: E0621 13:38:42.698617 2590595 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp 44.205.64.79:443: i/o timeout" image="busybox:latest"
Jun 21 13:38:42 cluster-2 kubelet[2590595]: E0621 13:38:42.698669 2590595 kuberuntime_manager.go:872] init container &Container{Name:init-ds,Image:busybox:latest,Command:[/bin/sh -c while true
Jun 21 13:38:42 cluster-2 kubelet[2590595]: do
Jun 21 13:38:42 cluster-2 kubelet[2590595]:   rt=$(nc -z -w 1 appbackend-postgresql 5432)
Jun 21 13:38:42 cluster-2 kubelet[2590595]:   if [ $? -eq 0 ]; then
Jun 21 13:38:42 cluster-2 kubelet[2590595]:     echo "DB is UP"
Jun 21 13:38:42 cluster-2 kubelet[2590595]:     break
Jun 21 13:38:42 cluster-2 kubelet[2590595]:   fi
Jun 21 13:38:42 cluster-2 kubelet[2590595]:   echo "DB is not yet reachable;sleep for 10s before retry"
Jun 21 13:38:42 cluster-2 kubelet[2590595]:   sleep 10
Jun 21 13:38:42 cluster-2 kubelet[2590595]: done
Jun 21 13:38:42 cluster-2 kubelet[2590595]: ],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb7ml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod appbackend-98c79665c-57qbh_audit-log(37091fe9-cb14-4c45-95c3-0dd1376ac2fe): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: Get "https://registry-1.docker.io/v2/": dial tcp 44.205.64.79:443: i/o timeout
Jun 21 13:38:42 cluster-2 kubelet[2590595]: E0621 13:38:42.698689 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": dial tcp 44.205.64.79:443: i/o timeout\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:38:53 cluster-2 kubelet[2590595]: I0621 13:38:53.678358 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:38:53 cluster-2 kubelet[2590595]: I0621 13:38:53.678519 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:38:53 cluster-2 kubelet[2590595]: E0621 13:38:53.682086 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:38:53 cluster-2 kubelet[2590595]: E0621 13:38:53.682361 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:39:04 cluster-2 kubelet[2590595]: I0621 13:39:04.676363 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:39:04 cluster-2 kubelet[2590595]: E0621 13:39:04.681300 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:39:06 cluster-2 kubelet[2590595]: I0621 13:39:06.676308 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:39:06 cluster-2 kubelet[2590595]: E0621 13:39:06.678337 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:39:15 cluster-2 kubelet[2590595]: I0621 13:39:15.676221 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:39:15 cluster-2 kubelet[2590595]: E0621 13:39:15.679087 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:39:17 cluster-2 kubelet[2590595]: I0621 13:39:17.676254 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:39:17 cluster-2 kubelet[2590595]: E0621 13:39:17.678254 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:39:27 cluster-2 kubelet[2590595]: I0621 13:39:27.676051 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:39:27 cluster-2 kubelet[2590595]: E0621 13:39:27.678083 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:39:32 cluster-2 kubelet[2590595]: I0621 13:39:32.677827 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:39:32 cluster-2 kubelet[2590595]: E0621 13:39:32.680215 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:39:41 cluster-2 kubelet[2590595]: I0621 13:39:41.676645 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:39:41 cluster-2 kubelet[2590595]: E0621 13:39:41.678212 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:39:44 cluster-2 kubelet[2590595]: I0621 13:39:44.675984 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appfrontend-75bd6f9fc5-qg7xt" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:39:47 cluster-2 kubelet[2590595]: I0621 13:39:47.676612 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:39:47 cluster-2 kubelet[2590595]: E0621 13:39:47.678673 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:39:53 cluster-2 kubelet[2590595]: I0621 13:39:53.676069 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:39:53 cluster-2 kubelet[2590595]: E0621 13:39:53.678016 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe
Jun 21 13:40:00 cluster-2 kubelet[2590595]: I0621 13:40:00.676600 2590595 scope.go:115] "RemoveContainer" containerID="581a8f291e62ac832c274a24c1e296fc1ecfba55ee22311fde6906abc539036a"
Jun 21 13:40:00 cluster-2 kubelet[2590595]: E0621 13:40:00.678757 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"jaeger\" with ImagePullBackOff: \"Back-off pulling image \\\"jaegertracing/all-in-one:latest\\\"\"" pod="default/jaeger-75557dfd79-ctfrk" podUID=19f97cf9-1d75-4ec4-8ac0-888d7c0ced60
Jun 21 13:40:06 cluster-2 kubelet[2590595]: I0621 13:40:06.676082 2590595 kubelet_pods.go:897] "Unable to retrieve pull secret, the image pull may not succeed." pod="audit-log/appbackend-98c79665c-57qbh" secret="" err="secret \"regcred-auditlogcollector\" not found"
Jun 21 13:40:06 cluster-2 kubelet[2590595]: E0621 13:40:06.678745 2590595 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"init-ds\" with ImagePullBackOff: \"Back-off pulling image \\\"busybox:latest\\\"\"" pod="audit-log/appbackend-98c79665c-57qbh" podUID=37091fe9-cb14-4c45-95c3-0dd1376ac2fe

* 
* ==> storage-provisioner [5c3e33f0ca94] <==
* I0621 13:31:23.347921       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0621 13:31:23.354590       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0621 13:31:23.354621       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0621 13:31:40.750484       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0621 13:31:40.750987       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_cluster-2_e87cd278-b5f2-43f4-8f52-da3a1dfb4eb4!
I0621 13:31:40.751121       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"cf551c5b-3961-442d-b12b-a55d33e386e7", APIVersion:"v1", ResourceVersion:"142471", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' cluster-2_e87cd278-b5f2-43f4-8f52-da3a1dfb4eb4 became leader
I0621 13:31:40.852464       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_cluster-2_e87cd278-b5f2-43f4-8f52-da3a1dfb4eb4!
I0621 13:31:40.852743       1 controller.go:1472] delete "pvc-2f29307e-5969-4cc2-ab37-718e7d37e6ca": started
I0621 13:31:40.852743       1 controller.go:1472] delete "pvc-c7e2245e-1f49-47b9-9fa7-0bdfd48b59c3": started
I0621 13:31:40.853273       1 controller.go:1472] delete "pvc-4b3410e7-d032-4db7-9e92-a9c838bfe824": started
I0621 13:31:40.852827       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-2f29307e-5969-4cc2-ab37-718e7d37e6ca    62aeb62d-9822-499d-aa83-f05456dd91c2 38261 0 2023-06-12 14:36:29 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:5f2e93e7-cabe-4ab0-a5ac-822da7e58a69 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2023-06-12 14:36:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2023-06-16 12:14:45 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/audit-log/appbackend-postgresql-pvc,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:audit-log,Name:appbackend-postgresql-pvc,UID:2f29307e-5969-4cc2-ab37-718e7d37e6ca,APIVersion:v1,ResourceVersion:4996,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0621 13:31:40.853744       1 controller.go:1478] delete "pvc-2f29307e-5969-4cc2-ab37-718e7d37e6ca": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0621 13:31:40.853278       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-4b3410e7-d032-4db7-9e92-a9c838bfe824    db749e6c-7340-4ce9-a565-15f1292b8b9f 56467 0 2023-06-16 12:17:28 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:8933bf79-a3f7-45db-9f9e-18afef4caafc pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2023-06-16 12:17:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2023-06-18 13:00:10 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/audit-log/appbackend-postgresql-pvc,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:audit-log,Name:appbackend-postgresql-pvc,UID:4b3410e7-d032-4db7-9e92-a9c838bfe824,APIVersion:v1,ResourceVersion:38788,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0621 13:31:40.853855       1 controller.go:1478] delete "pvc-4b3410e7-d032-4db7-9e92-a9c838bfe824": volume deletion ignored: ignored because identity annotation on PV does not match ours
I0621 13:31:40.853250       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-c7e2245e-1f49-47b9-9fa7-0bdfd48b59c3    d05cdd58-f634-4d37-a8e8-d30e769cb428 4883 0 2023-06-12 14:09:58 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:2a8832ba-a975-4c40-84ac-bc39af96c0db pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{storage-provisioner Update v1 2023-06-12 14:09:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{},"f:namespace":{},"f:resourceVersion":{},"f:uid":{}},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2023-06-12 14:35:27 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/audit-log/appbackend-postgresql-pvc,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:audit-log,Name:appbackend-postgresql-pvc,UID:c7e2245e-1f49-47b9-9fa7-0bdfd48b59c3,APIVersion:v1,ResourceVersion:2940,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0621 13:31:40.854234       1 controller.go:1478] delete "pvc-c7e2245e-1f49-47b9-9fa7-0bdfd48b59c3": volume deletion ignored: ignored because identity annotation on PV does not match ours

* 
* ==> storage-provisioner [96d767e042eb] <==
* I0621 13:30:56.668551       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0621 13:30:56.669412       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

